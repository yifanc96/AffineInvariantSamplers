{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a6e3f6-249f-4944-9b65-eb1e2edbd036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">benchmark_results_LWM_EKM_Allen-Cahn_test_20250822-223731</strong> at: <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/ix8hugxw' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/ix8hugxw</a><br> View project at: <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250822_223731-ix8hugxw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yc3400/scratch_research/gitrepo/AffineInvariantSamplers/github/AffineInvariantSamplers/other-samplers/Langevin-and-Kalman-move/wandb/run-20250822_223913-yasn9gj5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yifanc96/AffineInvariant/runs/yasn9gj5' target=\"_blank\">benchmark_results_LWM_EKM_Allen-Cahn_test_20250822-223913</a></strong> to <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/yasn9gj5' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/yasn9gj5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sample10000, burn_in1000, n_thin1\n",
      "dim=4\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.505163\n",
      "  Acceptance rate: 0.58\n",
      "  Path integral mean: -0.0009\n",
      "  Path integral std: 0.8726\n",
      "  Well mixing rate: 0.3410\n",
      "  Integrated autocorrelation time: 9.68\n",
      "  Time: 9.34 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.559908\n",
      "  Acceptance rate: 0.54\n",
      "  Path integral mean: -0.0065\n",
      "  Path integral std: 0.8703\n",
      "  Well mixing rate: 0.3373\n",
      "  Integrated autocorrelation time: 10.21\n",
      "  Time: 6.51 seconds\n",
      "dim=8\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.389049\n",
      "  Acceptance rate: 0.57\n",
      "  Path integral mean: -0.0009\n",
      "  Path integral std: 0.8594\n",
      "  Well mixing rate: 0.3346\n",
      "  Integrated autocorrelation time: 14.49\n",
      "  Time: 9.78 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.375502\n",
      "  Acceptance rate: 0.56\n",
      "  Path integral mean: 0.0011\n",
      "  Path integral std: 0.8533\n",
      "  Well mixing rate: 0.3340\n",
      "  Integrated autocorrelation time: 18.80\n",
      "  Time: 6.86 seconds\n",
      "dim=16\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.300636\n",
      "  Acceptance rate: 0.57\n",
      "  Path integral mean: -0.0040\n",
      "  Path integral std: 0.8438\n",
      "  Well mixing rate: 0.3285\n",
      "  Integrated autocorrelation time: 19.14\n",
      "  Time: 11.62 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.269276\n",
      "  Acceptance rate: 0.58\n",
      "  Path integral mean: -0.0054\n",
      "  Path integral std: 0.8431\n",
      "  Well mixing rate: 0.3285\n",
      "  Integrated autocorrelation time: 28.65\n",
      "  Time: 8.49 seconds\n",
      "dim=32\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.238333\n",
      "  Acceptance rate: 0.57\n",
      "  Path integral mean: 0.0001\n",
      "  Path integral std: 0.8372\n",
      "  Well mixing rate: 0.3281\n",
      "  Integrated autocorrelation time: 20.27\n",
      "  Time: 16.17 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.217410\n",
      "  Acceptance rate: 0.57\n",
      "  Path integral mean: -0.0022\n",
      "  Path integral std: 0.8395\n",
      "  Well mixing rate: 0.3280\n",
      "  Integrated autocorrelation time: 27.91\n",
      "  Time: 12.78 seconds\n",
      "dim=64\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.188579\n",
      "  Acceptance rate: 0.58\n",
      "  Path integral mean: 0.0048\n",
      "  Path integral std: 0.8340\n",
      "  Well mixing rate: 0.3243\n",
      "  Integrated autocorrelation time: 30.70\n",
      "  Time: 30.91 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.175106\n",
      "  Acceptance rate: 0.57\n",
      "  Path integral mean: -0.0081\n",
      "  Path integral std: 0.8336\n",
      "  Well mixing rate: 0.3233\n",
      "  Integrated autocorrelation time: 26.61\n",
      "  Time: 30.76 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "def autocorrelation_fft(x, max_lag=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Efficiently compute autocorrelation function using FFT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    max_lag : int, optional\n",
    "        Maximum lag to compute (default: len(x)//3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if max_lag is None:\n",
    "        max_lag = min(n // 3, 20000)  # Cap at 20000 to prevent slow computation\n",
    "    \n",
    "    # Remove mean and normalize\n",
    "    x_norm = x - np.mean(x)\n",
    "    var = np.var(x_norm)\n",
    "    x_norm = x_norm / np.sqrt(var)\n",
    "    \n",
    "    # Compute autocorrelation using FFT\n",
    "    # Pad the signal with zeros to avoid circular correlation\n",
    "    fft = np.fft.fft(x_norm, n=2*n)\n",
    "    acf = np.fft.ifft(fft * np.conjugate(fft))[:n]\n",
    "    acf = acf.real / n  # Normalize\n",
    "    \n",
    "    return acf[:max_lag]\n",
    "\n",
    "def integrated_autocorr_time(x, M=5, c=10):\n",
    "    \"\"\"\n",
    "    Estimate the integrated autocorrelation time using a self-consistent window.\n",
    "    Based on the algorithm described by Goodman and Weare.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    M : int, default=5\n",
    "        Window size multiplier (typically 5-10)\n",
    "    c : int, default=10\n",
    "        Maximum lag cutoff for window determination\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tau : float\n",
    "        Integrated autocorrelation time\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    ess : float\n",
    "        Effective sample size\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    orig_x = x.copy()\n",
    "    \n",
    "    # Initial pairwise reduction if needed\n",
    "    k = 0\n",
    "    max_iterations = 10  # Prevent infinite loop\n",
    "    \n",
    "    while k < max_iterations:\n",
    "        # Calculate autocorrelation function\n",
    "        acf = autocorrelation_fft(x)\n",
    "        \n",
    "        # Calculate integrated autocorrelation time with self-consistent window\n",
    "        tau = 1.0  # Initialize with the first term\n",
    "        \n",
    "        # Find the window size where window <= M * tau\n",
    "        for window in range(1, len(acf)):\n",
    "            # Update tau with this window\n",
    "            tau_window = 1.0 + 2.0 * sum(acf[1:window+1])\n",
    "            \n",
    "            # Check window consistency: window <= M*tau\n",
    "            if window <= M * tau_window:\n",
    "                tau = tau_window\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # If we have a robust estimate, we're done\n",
    "        if n >= c * tau:\n",
    "            # Scale tau back to the original time scale: tau_0 = 2^k * tau_k\n",
    "            tau = tau * (2**k)\n",
    "            break\n",
    "            \n",
    "        # If we don't have a robust estimate, perform pairwise reduction\n",
    "        k += 1\n",
    "        n_half = len(x) // 2\n",
    "        x_new = np.zeros(n_half)\n",
    "        for i in range(n_half):\n",
    "            if 2*i + 1 < len(x):\n",
    "                x_new[i] = 0.5 * (x[2*i] + x[2*i+1])\n",
    "            else:\n",
    "                x_new[i] = x[2*i]\n",
    "        x = x_new\n",
    "        n = len(x)\n",
    "    \n",
    "    # If we exited without a robust estimate, compute one final estimate\n",
    "    if k >= max_iterations or n < c * tau:\n",
    "        acf = autocorrelation_fft(orig_x)\n",
    "        tau_reduced = 1.0 + 2.0 * sum(acf[1:min(len(acf), int(M)+1)])\n",
    "        # Scale tau back to the original time scale\n",
    "        tau = tau_reduced * (2**k)\n",
    "    \n",
    "    # Calculate effective sample size using original series length\n",
    "    ess = len(orig_x) / tau\n",
    "    \n",
    "    return tau, acf, ess\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def langevin_walk_move_ensemble_dual_avg(gradient_func, potential_func, initial, n_samples, n_chains_per_group=5, \n",
    "                                        h=0.01, n_thin=1, target_accept=0.57, n_warmup=1000,\n",
    "                                        gamma=0.05, t0=10, kappa=0.75):\n",
    "    \"\"\"\n",
    "    Vectorized Langevin Walk Move sampler using normalized ensemble preconditioning with dual averaging\n",
    "    for automatic step size adaptation.\n",
    "    \n",
    "    This version follows the mathematical description exactly and includes dual averaging to tune\n",
    "    the step size to achieve a target acceptance rate during warmup.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gradient_func : callable\n",
    "        Function that computes the gradient of the potential V(x)\n",
    "    potential_func : callable\n",
    "        Function that computes the potential V(x)\n",
    "    initial : np.ndarray\n",
    "        Initial state\n",
    "    n_samples : int\n",
    "        Number of samples to collect (after warmup)\n",
    "    n_chains_per_group : int\n",
    "        Number of chains per group (default: 5)\n",
    "    h : float\n",
    "        Initial step size (default: 0.01)\n",
    "    n_thin : int\n",
    "        Thinning factor - store every n_thin sample (default: 1, no thinning)\n",
    "    target_accept : float\n",
    "        Target acceptance rate for dual averaging (default: 0.57)\n",
    "    n_warmup : int\n",
    "        Number of warmup iterations for step size adaptation (default: 1000)\n",
    "    gamma : float\n",
    "        Dual averaging parameter controlling adaptation rate (default: 0.05)\n",
    "    t0 : float\n",
    "        Dual averaging parameter for numerical stability (default: 10)\n",
    "    kappa : float\n",
    "        Dual averaging parameter controlling decay (default: 0.75, should be in (0.5, 1])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : np.ndarray\n",
    "        Collected samples from all chains (after warmup)\n",
    "    acceptance_rates : np.ndarray\n",
    "        Final acceptance rates for all chains\n",
    "    step_size_history : np.ndarray\n",
    "        History of step sizes during adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    orig_dim = initial.shape\n",
    "    flat_dim = np.prod(orig_dim)\n",
    "    total_chains = 2 * n_chains_per_group\n",
    "    \n",
    "    # Create initial states with small random perturbations\n",
    "    states = np.tile(initial.flatten(), (total_chains, 1)) + 0.1 * np.random.randn(total_chains, flat_dim)\n",
    "    \n",
    "    # Split into two groups\n",
    "    group1 = slice(0, n_chains_per_group)\n",
    "    group2 = slice(n_chains_per_group, total_chains)\n",
    "    \n",
    "    # Dual averaging initialization\n",
    "    log_h = np.log(h)\n",
    "    log_h_bar = 0.0\n",
    "    h_bar = 0.0\n",
    "    step_size_history = []\n",
    "    \n",
    "    # Calculate total iterations needed based on thinning factor\n",
    "    total_sampling_iterations = n_samples * n_thin\n",
    "    total_iterations = n_warmup + total_sampling_iterations\n",
    "    \n",
    "    # Storage for samples and acceptance tracking\n",
    "    samples = np.zeros((total_chains, n_samples, flat_dim))\n",
    "    accepts_warmup = np.zeros(total_chains)  # Track accepts during warmup\n",
    "    accepts_sampling = np.zeros(total_chains)  # Track accepts during sampling\n",
    "    \n",
    "    # Sample index to track where to store thinned samples\n",
    "    sample_idx = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    for i in range(total_iterations):\n",
    "        is_warmup = i < n_warmup\n",
    "        current_h = h if not is_warmup else np.exp(log_h)\n",
    "        \n",
    "        # Store current state from all chains (only during sampling phase)\n",
    "        if not is_warmup and (i - n_warmup) % n_thin == 0 and sample_idx < n_samples:\n",
    "            samples[:, sample_idx] = states\n",
    "            sample_idx += 1\n",
    "        \n",
    "        # Compute normalized centered ensemble from group 2 for group 1 update\n",
    "        # B_S has shape (flat_dim, n_chains_per_group)\n",
    "        group2_centered = states[group2] - np.mean(states[group2], axis=0)\n",
    "        B_S2 = (group2_centered / np.sqrt(n_chains_per_group)).T  # (flat_dim, n_chains_per_group)\n",
    "        \n",
    "        # First group update\n",
    "        current_q1 = states[group1].copy()\n",
    "        current_q1_reshaped = current_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U1 = potential_func(current_q1_reshaped)\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad1 = gradient_func(current_q1_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "        \n",
    "        # Generate noise in ensemble space (dimension n_chains_per_group)\n",
    "        z1 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        # Langevin proposal: x_i = x_i - h * B_S * B_S^T * ∇V(x_i) + √(2h) * B_S * z_i\n",
    "        # where z_i has dimension n_chains_per_group\n",
    "        \n",
    "        # Compute B_S^T * B_S (covariance in parameter space)\n",
    "        cov_param2 = np.dot(B_S2, B_S2.T)  # (flat_dim, flat_dim)\n",
    "        \n",
    "        # Drift term: -h * B_S * B_S^T * ∇V\n",
    "        drift_term = -current_h * (cov_param2 @ grad1.T).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        # Noise term: √(2h) * B_S * z\n",
    "        noise_term = np.sqrt(2 * current_h) * (B_S2 @ z1.T).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        proposed_q1 = current_q1 + drift_term + noise_term\n",
    "        \n",
    "        # Compute proposed energy\n",
    "        proposed_q1_reshaped = proposed_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        proposed_U1 = potential_func(proposed_q1_reshaped)\n",
    "        \n",
    "        # Metropolis-Hastings acceptance with correct proposal probabilities\n",
    "        grad1_proposed = gradient_func(proposed_q1_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad1_proposed = np.nan_to_num(grad1_proposed, nan=0.0)\n",
    "        \n",
    "        # Forward and reverse proposal means\n",
    "        mean_forward = current_q1 - current_h * (cov_param2 @ grad1.T).T\n",
    "        mean_reverse = proposed_q1 - current_h * (cov_param2 @ grad1_proposed.T).T\n",
    "        \n",
    "        # Compute residuals\n",
    "        residual_forward = proposed_q1 - mean_forward\n",
    "        residual_reverse = current_q1 - mean_reverse\n",
    "        \n",
    "        # Compute quadratic forms with covariance 2h * B_S * B_S^T\n",
    "        # The inverse covariance is (2h * B_S * B_S^T)^(-1) = (1/2h) * (B_S * B_S^T)^(-1)\n",
    "        \n",
    "        try:\n",
    "            # Add regularization for numerical stability\n",
    "            reg_cov2 = cov_param2 + 1e-8 * np.eye(flat_dim)\n",
    "            L2 = np.linalg.cholesky(reg_cov2)\n",
    "            \n",
    "            # Solve for quadratic forms: (1/2h) * residual^T * (B_S * B_S^T)^(-1) * residual\n",
    "            Y_forward = np.linalg.solve(L2, residual_forward.T)  # (flat_dim, n_chains_per_group)\n",
    "            Y_reverse = np.linalg.solve(L2, residual_reverse.T)  # (flat_dim, n_chains_per_group)\n",
    "            \n",
    "            # Quadratic forms: (1/(4h)) * residual^T * (B_S * B_S^T)^(-1) * residual\n",
    "            # Note: factor is 1/(4h), not 1/(2h), to match covariance 2h * B_S * B_S^T\n",
    "            log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "            log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback to pseudoinverse\n",
    "            inv_cov2 = np.linalg.pinv(cov_param2)\n",
    "            \n",
    "            log_q_forward = np.zeros(n_chains_per_group)\n",
    "            log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            for j in range(n_chains_per_group):\n",
    "                log_q_forward[j] = -residual_forward[j] @ inv_cov2 @ residual_forward[j] / (4 * current_h)\n",
    "                log_q_reverse[j] = -residual_reverse[j] @ inv_cov2 @ residual_reverse[j] / (4 * current_h)\n",
    "        \n",
    "        # Metropolis-Hastings ratio\n",
    "        dU1 = proposed_U1 - current_U1\n",
    "        log_ratio = -dU1 + log_q_reverse - log_q_forward\n",
    "        \n",
    "        # Accept/reject with numerical stability\n",
    "        accept_probs1 = np.ones_like(log_ratio)\n",
    "        exp_needed = log_ratio < 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "            accept_probs1[exp_needed] = np.exp(safe_log_ratio)\n",
    "        \n",
    "        accepts1 = np.random.random(n_chains_per_group) < accept_probs1\n",
    "        states[group1][accepts1] = proposed_q1[accepts1]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group1] += accepts1\n",
    "        else:\n",
    "            accepts_sampling[group1] += accepts1\n",
    "        \n",
    "        # Second group update using ensemble from group 1\n",
    "        group1_centered = states[group1] - np.mean(states[group1], axis=0)\n",
    "        B_S1 = (group1_centered / np.sqrt(n_chains_per_group)).T  # (flat_dim, n_chains_per_group)\n",
    "        \n",
    "        current_q2 = states[group2].copy()\n",
    "        current_q2_reshaped = current_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U2 = potential_func(current_q2_reshaped)\n",
    "        \n",
    "        grad2 = gradient_func(current_q2_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "        \n",
    "        z2 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        cov_param1 = np.dot(B_S1, B_S1.T)  # (flat_dim, flat_dim)\n",
    "        \n",
    "        drift_term = -current_h * (cov_param1 @ grad2.T).T\n",
    "        noise_term = np.sqrt(2 * current_h) * (B_S1 @ z2.T).T\n",
    "        \n",
    "        proposed_q2 = current_q2 + drift_term + noise_term\n",
    "        \n",
    "        proposed_q2_reshaped = proposed_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        proposed_U2 = potential_func(proposed_q2_reshaped)\n",
    "        \n",
    "        grad2_proposed = gradient_func(proposed_q2_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad2_proposed = np.nan_to_num(grad2_proposed, nan=0.0)\n",
    "        \n",
    "        mean_forward = current_q2 - current_h * (cov_param1 @ grad2.T).T\n",
    "        mean_reverse = proposed_q2 - current_h * (cov_param1 @ grad2_proposed.T).T\n",
    "        \n",
    "        residual_forward = proposed_q2 - mean_forward\n",
    "        residual_reverse = current_q2 - mean_reverse\n",
    "        \n",
    "        try:\n",
    "            reg_cov1 = cov_param1 + 1e-8 * np.eye(flat_dim)\n",
    "            L1 = np.linalg.cholesky(reg_cov1)\n",
    "            \n",
    "            Y_forward = np.linalg.solve(L1, residual_forward.T)\n",
    "            Y_reverse = np.linalg.solve(L1, residual_reverse.T)\n",
    "            \n",
    "            log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "            log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            inv_cov1 = np.linalg.pinv(cov_param1)\n",
    "            \n",
    "            log_q_forward = np.zeros(n_chains_per_group)\n",
    "            log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            for j in range(n_chains_per_group):\n",
    "                log_q_forward[j] = -residual_forward[j] @ inv_cov1 @ residual_forward[j] / (4 * current_h)\n",
    "                log_q_reverse[j] = -residual_reverse[j] @ inv_cov1 @ residual_reverse[j] / (4 * current_h)\n",
    "        \n",
    "        dU2 = proposed_U2 - current_U2\n",
    "        log_ratio = -dU2 + log_q_reverse - log_q_forward\n",
    "        \n",
    "        accept_probs2 = np.ones_like(log_ratio)\n",
    "        exp_needed = log_ratio < 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "            accept_probs2[exp_needed] = np.exp(safe_log_ratio)\n",
    "        \n",
    "        accepts2 = np.random.random(n_chains_per_group) < accept_probs2\n",
    "        states[group2][accepts2] = proposed_q2[accepts2]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group2] += accepts2\n",
    "        else:\n",
    "            accepts_sampling[group2] += accepts2\n",
    "        \n",
    "        # Dual averaging step size adaptation during warmup\n",
    "        if is_warmup:\n",
    "            # Average acceptance probability across all chains in this iteration\n",
    "            current_accept_rate = (np.sum(accepts1) + np.sum(accepts2)) / total_chains\n",
    "            \n",
    "            # Dual averaging update\n",
    "            m = i + 1  # iteration number (1-indexed)\n",
    "            eta_m = 1.0 / (m + t0)\n",
    "            \n",
    "            # Update log step size\n",
    "            h_bar = (1 - eta_m) * h_bar + eta_m * (target_accept - current_accept_rate)\n",
    "            \n",
    "            # Compute log step size with shrinkage\n",
    "            log_h = np.log(h) - np.sqrt(m) / gamma * h_bar\n",
    "            \n",
    "            # Update log_h_bar for final step size\n",
    "            eta_bar_m = m**(-kappa)\n",
    "            log_h_bar = (1 - eta_bar_m) * log_h_bar + eta_bar_m * log_h\n",
    "            \n",
    "            # Store step size history\n",
    "            step_size_history.append(np.exp(log_h))\n",
    "        \n",
    "        # After warmup, fix step size to the adapted value\n",
    "        if i == n_warmup - 1:\n",
    "            h = np.exp(log_h_bar)\n",
    "            print(f\"Warmup complete. Final adapted step size: {h:.6f}\")\n",
    "    \n",
    "    # Reshape final samples to original dimensions\n",
    "    samples = samples.reshape((total_chains, n_samples) + orig_dim)\n",
    "    \n",
    "    # Compute acceptance rates for sampling phase only\n",
    "    acceptance_rates = accepts_sampling / total_sampling_iterations\n",
    "    \n",
    "    return samples, acceptance_rates, np.array(step_size_history)\n",
    "\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "def ensemble_kalman_move_dual_avg(forward_func, initial, n_samples, M=None,\n",
    "                                 n_chains_per_group=5, h=0.01, n_thin=1, use_metropolis=True,\n",
    "                                 target_accept=0.57, n_warmup=1000, gamma=0.05, t0=10, kappa=0.75):\n",
    "    \"\"\"\n",
    "    Ensemble Kalman Move sampler for least squares type densities with dual averaging\n",
    "    for automatic step size adaptation.\n",
    "    \n",
    "    For density π(x) ∝ exp(-V(x)) with V(x) = ½G(x)ᵀMG(x) where G: ℝᵈ → ℝʳ.\n",
    "    \n",
    "    The proposal is: x' = x - h * B_S * F_S^T * M * G(x) + √(2h) * B_S * z\n",
    "    where:\n",
    "    - B_S: normalized centered ensemble from other group (flat_dim, n_chains_per_group)\n",
    "    - F_S: normalized centered G(x) from other group (data_dim, n_chains_per_group)  \n",
    "    - z ~ N(0, I_{n_chains_per_group × n_chains_per_group})\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    forward_func : callable\n",
    "        Function G(x) that maps parameters to data space ℝᵈ → ℝʳ\n",
    "        Must accept batch input: G(x_batch) where x_batch has shape (n_batch, *param_shape)\n",
    "        Returns array of shape (n_batch, data_dim)\n",
    "    initial : np.ndarray\n",
    "        Initial state\n",
    "    n_samples : int\n",
    "        Number of samples to collect (after warmup)\n",
    "    M : np.ndarray, optional\n",
    "        Precision matrix in data space (default: identity)\n",
    "    n_chains_per_group : int\n",
    "        Number of chains per group (default: 5)\n",
    "    h : float\n",
    "        Initial step size (default: 0.01)\n",
    "    n_thin : int\n",
    "        Thinning factor (default: 1, no thinning)\n",
    "    use_metropolis : bool\n",
    "        Whether to use Metropolis correction for exact sampling (default: True)\n",
    "    target_accept : float\n",
    "        Target acceptance rate for dual averaging (default: 0.57)\n",
    "    n_warmup : int\n",
    "        Number of warmup iterations for step size adaptation (default: 1000)\n",
    "    gamma : float\n",
    "        Dual averaging parameter controlling adaptation rate (default: 0.05)\n",
    "    t0 : float\n",
    "        Dual averaging parameter for numerical stability (default: 10)\n",
    "    kappa : float\n",
    "        Dual averaging parameter controlling decay (default: 0.75, should be in (0.5, 1])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : np.ndarray\n",
    "        Collected samples from all chains (after warmup)\n",
    "    acceptance_rates : np.ndarray\n",
    "        Final acceptance rates for all chains\n",
    "    step_size_history : np.ndarray\n",
    "        History of step sizes during adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    orig_dim = initial.shape\n",
    "    flat_dim = np.prod(orig_dim)\n",
    "    total_chains = 2 * n_chains_per_group\n",
    "    \n",
    "    # Create initial states with small random perturbations\n",
    "    states = np.tile(initial.flatten(), (total_chains, 1)) + 0.1 * np.random.randn(total_chains, flat_dim)\n",
    "    \n",
    "    # Split into two groups\n",
    "    group1 = slice(0, n_chains_per_group)\n",
    "    group2 = slice(n_chains_per_group, total_chains)\n",
    "    \n",
    "    # Dual averaging initialization\n",
    "    log_h = np.log(h)\n",
    "    log_h_bar = 0.0\n",
    "    h_bar = 0.0\n",
    "    step_size_history = []\n",
    "    \n",
    "    # Calculate total iterations needed based on thinning factor\n",
    "    total_sampling_iterations = n_samples * n_thin\n",
    "    total_iterations = n_warmup + total_sampling_iterations\n",
    "    \n",
    "    # Storage for samples and acceptance tracking\n",
    "    samples = np.zeros((total_chains, n_samples, flat_dim))\n",
    "    accepts_warmup = np.zeros(total_chains)  # Track accepts during warmup\n",
    "    accepts_sampling = np.zeros(total_chains)  # Track accepts during sampling\n",
    "    \n",
    "    # Sample index to track where to store thinned samples\n",
    "    sample_idx = 0\n",
    "    \n",
    "    # Determine data dimension from first forward model evaluation\n",
    "    test_G = forward_func(initial)\n",
    "    data_dim = len(test_G)\n",
    "    \n",
    "    # Set default M = I if not provided\n",
    "    if M is None:\n",
    "        M = np.eye(data_dim)\n",
    "    \n",
    "    # Main sampling loop\n",
    "    for i in range(total_iterations):\n",
    "        is_warmup = i < n_warmup\n",
    "        current_h = h if not is_warmup else np.exp(log_h)\n",
    "        \n",
    "        # Store current state from all chains (only during sampling phase)\n",
    "        if not is_warmup and (i - n_warmup) % n_thin == 0 and sample_idx < n_samples:\n",
    "            samples[:, sample_idx] = states\n",
    "            sample_idx += 1\n",
    "        \n",
    "        # Update group 1 using group 2 for ensemble information\n",
    "        group2_reshaped = states[group2].reshape(n_chains_per_group, *orig_dim)\n",
    "        G_group2 = forward_func(group2_reshaped)  # (n_chains_per_group, data_dim)\n",
    "        mean_G2 = np.mean(G_group2, axis=0)  # (data_dim,)\n",
    "        \n",
    "        # F_S1 = (1/√(N/2)) * [G(x_group2) - mean_G2]^T ∈ ℝ^{data_dim × n_chains_per_group}\n",
    "        F_S1 = ((G_group2 - mean_G2) / np.sqrt(n_chains_per_group)).T  # (data_dim, n_chains_per_group)\n",
    "        \n",
    "        # B_S1 from group 2: normalized centered ensemble in parameter space\n",
    "        group2_centered = states[group2] - np.mean(states[group2], axis=0)\n",
    "        B_S1 = (group2_centered / np.sqrt(n_chains_per_group)).T  # (flat_dim, n_chains_per_group)\n",
    "        \n",
    "        # Vectorized update for group 1\n",
    "        current_q1 = states[group1].copy()\n",
    "        current_q1_reshaped = current_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        G_current1 = forward_func(current_q1_reshaped)  # (n_chains_per_group, data_dim)\n",
    "        current_U1 = 0.5 * np.sum(G_current1 * (G_current1 @ M), axis=1)  # (n_chains_per_group,)\n",
    "        \n",
    "        # Generate noise z ~ N(0, I_{n_chains_per_group × n_chains_per_group})\n",
    "        z1 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        # Ensemble Kalman proposal:\n",
    "        # x' = x - h * B_S1 * F_S1^T * M * G(x) + √(2h) * B_S1 * z\n",
    "        \n",
    "        # Drift term: -h * B_S1 * F_S1^T * M * G(x) (vectorized)\n",
    "        MG_current1 = G_current1 @ M  # (n_chains_per_group, data_dim)\n",
    "        drift_terms = -current_h * (B_S1 @ (F_S1.T @ MG_current1.T)).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        # Noise term: √(2h) * B_S1 * z (vectorized)\n",
    "        noise_terms = np.sqrt(2 * current_h) * (B_S1 @ z1.T).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        # Proposed states (vectorized)\n",
    "        proposed_q1 = current_q1 + drift_terms + noise_terms\n",
    "        \n",
    "        accepts1 = np.zeros(n_chains_per_group, dtype=bool)\n",
    "        \n",
    "        if use_metropolis:\n",
    "            # Metropolis-Hastings correction\n",
    "            proposed_q1_reshaped = proposed_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "            G_proposed1 = forward_func(proposed_q1_reshaped)\n",
    "            proposed_U1 = 0.5 * np.sum(G_proposed1 * (G_proposed1 @ M), axis=1)\n",
    "            \n",
    "            # For correct Metropolis step, we need to compute proposal probabilities\n",
    "            # The proposal has the form: x' ~ N(μ_forward, Σ) where:\n",
    "            # μ_forward = x - h * B_S1 * F_S1^T * M * G(x)\n",
    "            # Σ = 2h * B_S1 * B_S1^T (covariance of noise term)\n",
    "            \n",
    "            # IMPORTANT: F_S1 is FIXED (from group 2) during this proposal step\n",
    "            # So both forward and reverse proposals use the SAME F_S1\n",
    "            \n",
    "            # Forward proposal mean (what we used to generate proposed_q1)\n",
    "            mean_forward = current_q1 + drift_terms  # Note: drift_terms already has negative sign\n",
    "            \n",
    "            # Reverse proposal: proposed_q1 -> current_q1\n",
    "            # Mean: proposed_q1 - h * B_S1 * F_S1^T * M * G(proposed_q1)\n",
    "            # Note: Uses SAME F_S1 (from group 2), not recomputed!\n",
    "            MG_proposed1 = G_proposed1 @ M\n",
    "            reverse_drift = -current_h * (B_S1 @ (F_S1.T @ MG_proposed1.T)).T  # Same F_S1!\n",
    "            mean_reverse = proposed_q1 + reverse_drift\n",
    "            \n",
    "            # Compute residuals\n",
    "            residual_forward = proposed_q1 - mean_forward  # Should be = noise_terms\n",
    "            residual_reverse = current_q1 - mean_reverse\n",
    "            \n",
    "            # Proposal covariance: The noise term √(2h) * B_S * z has covariance 2h * B_S * B_S^T\n",
    "            # But for the log probability calculation, we factor out the h:\n",
    "            # log q = -½ * residual^T * (2h * B_S * B_S^T)^(-1) * residual\n",
    "            #       = -1/(4h) * residual^T * (B_S * B_S^T)^(-1) * residual\n",
    "            # So we use cov = B_S * B_S^T (no h) and divide by 4h later\n",
    "            cov_proposal = np.dot(B_S1, B_S1.T)  # (flat_dim, flat_dim) - NO h here!\n",
    "            \n",
    "            try:\n",
    "                # Use Cholesky for numerical stability\n",
    "                reg_cov = cov_proposal + 1e-8 * np.eye(flat_dim)\n",
    "                L = np.linalg.cholesky(reg_cov)\n",
    "                \n",
    "                # Compute log proposal probabilities: log q(x' | x) = -½(x' - μ)^T Σ^(-1) (x' - μ)\n",
    "                # Since Σ = 2h * B_S1 * B_S1^T, we have:\n",
    "                # log q = -¼h * residual^T * (B_S1 * B_S1^T)^(-1) * residual\n",
    "                \n",
    "                # Solve L * y = residual^T for each chain\n",
    "                Y_forward = np.linalg.solve(L, residual_forward.T)  # (flat_dim, n_chains_per_group)\n",
    "                Y_reverse = np.linalg.solve(L, residual_reverse.T)   # (flat_dim, n_chains_per_group)\n",
    "                \n",
    "                # Log proposal probabilities (factor of 1/(4h) because Σ = 2h * ...)\n",
    "                log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "                log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "                \n",
    "            except np.linalg.LinAlgError:\n",
    "                # If Cholesky fails, use simpler Metropolis (energy only)\n",
    "                log_q_forward = np.zeros(n_chains_per_group)\n",
    "                log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            # Metropolis-Hastings ratio\n",
    "            dU1 = proposed_U1 - current_U1\n",
    "            log_ratio = -dU1 + log_q_reverse - log_q_forward\n",
    "            \n",
    "            # Accept/reject with numerical stability\n",
    "            accept_probs1 = np.ones_like(log_ratio)\n",
    "            exp_needed = log_ratio < 0\n",
    "            if np.any(exp_needed):\n",
    "                safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "                accept_probs1[exp_needed] = np.exp(safe_log_ratio)\n",
    "            \n",
    "            accepts1 = np.random.random(n_chains_per_group) < accept_probs1\n",
    "            states[group1][accepts1] = proposed_q1[accepts1]\n",
    "        else:\n",
    "            # Pure Ensemble Kalman (no Metropolis correction)\n",
    "            states[group1] = proposed_q1\n",
    "            accepts1 = np.ones(n_chains_per_group, dtype=bool)  # Always accept\n",
    "        \n",
    "        # Track acceptances for group 1\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group1] += accepts1\n",
    "        else:\n",
    "            accepts_sampling[group1] += accepts1\n",
    "        \n",
    "        # Update group 2 using group 1 (symmetric structure)\n",
    "        group1_reshaped = states[group1].reshape(n_chains_per_group, *orig_dim)\n",
    "        G_group1 = forward_func(group1_reshaped)\n",
    "        mean_G1 = np.mean(G_group1, axis=0)\n",
    "        F_S0 = ((G_group1 - mean_G1) / np.sqrt(n_chains_per_group)).T\n",
    "        \n",
    "        group1_centered = states[group1] - np.mean(states[group1], axis=0)\n",
    "        B_S0 = (group1_centered / np.sqrt(n_chains_per_group)).T\n",
    "        \n",
    "        current_q2 = states[group2].copy()\n",
    "        current_q2_reshaped = current_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        G_current2 = forward_func(current_q2_reshaped)\n",
    "        current_U2 = 0.5 * np.sum(G_current2 * (G_current2 @ M), axis=1)\n",
    "        \n",
    "        z2 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        MG_current2 = G_current2 @ M\n",
    "        drift_terms = -current_h * (B_S0 @ (F_S0.T @ MG_current2.T)).T\n",
    "        noise_terms = np.sqrt(2 * current_h) * (B_S0 @ z2.T).T\n",
    "        proposed_q2 = current_q2 + drift_terms + noise_terms\n",
    "        \n",
    "        accepts2 = np.zeros(n_chains_per_group, dtype=bool)\n",
    "        \n",
    "        if use_metropolis:\n",
    "            proposed_q2_reshaped = proposed_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "            G_proposed2 = forward_func(proposed_q2_reshaped)\n",
    "            proposed_U2 = 0.5 * np.sum(G_proposed2 * (G_proposed2 @ M), axis=1)\n",
    "            \n",
    "            mean_forward = current_q2 + drift_terms\n",
    "            \n",
    "            # Reverse proposal: proposed_q2 -> current_q2  \n",
    "            # Mean: proposed_q2 - h * B_S0 * F_S0^T * M * G(proposed_q2)\n",
    "            # Note: Uses SAME F_S0 (from group 1), not recomputed!\n",
    "            MG_proposed2 = G_proposed2 @ M\n",
    "            reverse_drift = -current_h * (B_S0 @ (F_S0.T @ MG_proposed2.T)).T  # Same F_S0!\n",
    "            mean_reverse = proposed_q2 + reverse_drift\n",
    "            \n",
    "            residual_forward = proposed_q2 - mean_forward\n",
    "            residual_reverse = current_q2 - mean_reverse\n",
    "            \n",
    "            cov_proposal = np.dot(B_S0, B_S0.T)  # NO h here!\n",
    "            \n",
    "            try:\n",
    "                reg_cov = cov_proposal + 1e-8 * np.eye(flat_dim)\n",
    "                L = np.linalg.cholesky(reg_cov)\n",
    "                \n",
    "                Y_forward = np.linalg.solve(L, residual_forward.T)\n",
    "                Y_reverse = np.linalg.solve(L, residual_reverse.T)\n",
    "                \n",
    "                log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "                log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "                \n",
    "            except np.linalg.LinAlgError:\n",
    "                log_q_forward = np.zeros(n_chains_per_group)\n",
    "                log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            dU2 = proposed_U2 - current_U2\n",
    "            log_ratio = -dU2 + log_q_reverse - log_q_forward\n",
    "            \n",
    "            accept_probs2 = np.ones_like(log_ratio)\n",
    "            exp_needed = log_ratio < 0\n",
    "            if np.any(exp_needed):\n",
    "                safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "                accept_probs2[exp_needed] = np.exp(safe_log_ratio)\n",
    "            \n",
    "            accepts2 = np.random.random(n_chains_per_group) < accept_probs2\n",
    "            states[group2][accepts2] = proposed_q2[accepts2]\n",
    "        else:\n",
    "            states[group2] = proposed_q2\n",
    "            accepts2 = np.ones(n_chains_per_group, dtype=bool)\n",
    "        \n",
    "        # Track acceptances for group 2\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group2] += accepts2\n",
    "        else:\n",
    "            accepts_sampling[group2] += accepts2\n",
    "        \n",
    "        # Dual averaging step size adaptation during warmup\n",
    "        if is_warmup:\n",
    "            # Average acceptance probability across all chains in this iteration\n",
    "            current_accept_rate = (np.sum(accepts1) + np.sum(accepts2)) / total_chains\n",
    "            \n",
    "            # Dual averaging update\n",
    "            m = i + 1  # iteration number (1-indexed)\n",
    "            eta_m = 1.0 / (m + t0)\n",
    "            \n",
    "            # Update log step size\n",
    "            h_bar = (1 - eta_m) * h_bar + eta_m * (target_accept - current_accept_rate)\n",
    "            \n",
    "            # Compute log step size with shrinkage\n",
    "            log_h = np.log(h) - np.sqrt(m) / gamma * h_bar\n",
    "            \n",
    "            # Update log_h_bar for final step size\n",
    "            eta_bar_m = m**(-kappa)\n",
    "            log_h_bar = (1 - eta_bar_m) * log_h_bar + eta_bar_m * log_h\n",
    "            \n",
    "            # Store step size history\n",
    "            step_size_history.append(np.exp(log_h))\n",
    "        \n",
    "        # After warmup, fix step size to the adapted value\n",
    "        if i == n_warmup - 1:\n",
    "            h = np.exp(log_h_bar)\n",
    "            print(f\"Warmup complete. Final adapted step size: {h:.6f}\")\n",
    "    \n",
    "    # Reshape final samples to original dimensions\n",
    "    samples = samples.reshape((total_chains, n_samples) + orig_dim)\n",
    "    \n",
    "    # Compute acceptance rates for sampling phase only\n",
    "    acceptance_rates = accepts_sampling / total_sampling_iterations\n",
    "    \n",
    "    return samples, acceptance_rates, np.array(step_size_history)\n",
    "\n",
    "def benchmark_samplers_allen_cahn(N=100, n_samples=10000, burn_in=1000, n_thin=1, save_dir=None):\n",
    "    \"\"\"\n",
    "    Benchmark different MCMC samplers on the invariant measure of the Allen-Cahn SPDE.\n",
    "    \n",
    "    The Allen-Cahn SPDE has the invariant measure with density proportional to:\n",
    "    exp(-∫[1/(2h) * (du/dx)² + V(u)] dx)\n",
    "    \n",
    "    where V(u) = (1 - u²)² is the double-well potential and h is the discretization step.\n",
    "    \"\"\"\n",
    "    # Define discretization parameters\n",
    "    h = 1.0 / N\n",
    "    dim = N + 1  # Including boundary points\n",
    "    \n",
    "    # Define potential function V'(u) = -4u(1 - u^2)\n",
    "    def V_prime(u):\n",
    "        return -4 * u * (1 - u**2)\n",
    "    \n",
    "    # Define the gradient of the negative log density - vectorized\n",
    "    def gradient(u):\n",
    "        \"\"\"Numerically stable vectorized gradient of the negative log density\"\"\"\n",
    "        if u.ndim == 1:\n",
    "            u = u.reshape(1, -1)\n",
    "            \n",
    "        grad = np.zeros_like(u)\n",
    "        \n",
    "        # Handle interior points (j=1 to j=N-1) with vectorization\n",
    "        u_prev = u[:, :-2]  # u[j-1] for j=1...N-1\n",
    "        u_curr = u[:, 1:-1]  # u[j] for j=1...N-1\n",
    "        u_next = u[:, 2:]    # u[j+1] for j=1...N-1\n",
    "        \n",
    "        # Coupling term contribution: (2*u[j] - u[j-1] - u[j+1])/h\n",
    "        coupling_term = (2 * u_curr - u_prev - u_next) / h\n",
    "        \n",
    "        # Potential term contribution\n",
    "        avg_prev = (u_curr + u_prev) / 2\n",
    "        avg_next = (u_curr + u_next) / 2\n",
    "        \n",
    "        v_prime_prev = -4 * avg_prev * (1 - avg_prev**2)\n",
    "        v_prime_next = -4 * avg_next * (1 - avg_next**2)\n",
    "        \n",
    "        potential_term = h * (v_prime_prev + v_prime_next) / 4\n",
    "        \n",
    "        # Combine contributions for interior points\n",
    "        grad[:, 1:-1] = coupling_term + potential_term\n",
    "        \n",
    "        # Handle boundary points\n",
    "        u_first = u[:, 0]\n",
    "        u_second = u[:, 1]\n",
    "        grad[:, 0] = (u_first - u_second) / h + h * V_prime(u_first) / 4\n",
    "        \n",
    "        u_last = u[:, -1]\n",
    "        u_second_last = u[:, -2]\n",
    "        grad[:, -1] = (u_last - u_second_last) / h + h * V_prime(u_last) / 4\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    # Define the potential energy function - vectorized\n",
    "    def potential(u):\n",
    "        \"\"\"Numerically stable vectorized negative log density (potential energy)\"\"\"\n",
    "        if u.ndim == 1:\n",
    "            u = u.reshape(1, -1)\n",
    "            \n",
    "        u_right = u[:, 1:]\n",
    "        u_left = u[:, :-1]\n",
    "        diffs = u_right - u_left\n",
    "        \n",
    "        coupling_term = np.sum(diffs**2, axis=1) / (2*h)\n",
    "        \n",
    "        u_avg = (u_right + u_left) / 2\n",
    "        v_values = (1 - u_avg**2)**2\n",
    "        potential_term = np.sum(h * v_values / 2, axis=1)\n",
    "        \n",
    "        total_potential = np.clip(coupling_term + potential_term, -1e10, 1e10)\n",
    "            \n",
    "        return total_potential\n",
    "\n",
    "    # def forward_func(u_batch):\n",
    "    #     \"\"\"\n",
    "    #     Forward model G(u) for Allen-Cahn SPDE where V(u) = ½|G(u)|²\n",
    "        \n",
    "    #     The Allen-Cahn energy is: ∫[½(∂ₓu)² + (1-u²)²] dx\n",
    "    #     We can write this as ½|G(u)|² where G(u) has components:\n",
    "    #     - Gradient contributions: (u[j+1] - u[j])/√h for j=0..N-1  \n",
    "    #     - Potential contributions: √(2h)(1-u[j]²) for j=0..N\n",
    "        \n",
    "    #     This gives equal weight to gradient and potential terms in the energy.\n",
    "    #     \"\"\"\n",
    "    #     if u_batch.ndim == 1:\n",
    "    #         u_batch = u_batch.reshape(1, -1)\n",
    "        \n",
    "    #     batch_size = u_batch.shape[0]\n",
    "        \n",
    "    #     # Gradient contributions: (u[j+1] - u[j])/√h for j=0..N-1\n",
    "    #     # This discretizes ∫(∂ₓu)² dx ≈ Σ (u[j+1] - u[j])²/h\n",
    "    #     u_diffs = u_batch[:, 1:] - u_batch[:, :-1]  # (batch_size, dim-1)\n",
    "    #     grad_part = u_diffs / np.sqrt(h)  # Scale by 1/√h\n",
    "        \n",
    "    #     # Potential contributions: √(2h)(1-u[j]²) for j=0..N  \n",
    "    #     # This discretizes ∫2(1-u²)² dx ≈ Σ 2h(1-u[j]²)²\n",
    "    #     nonlinear_part = np.sqrt(2*h) * (1 - u_batch**2)  # (batch_size, dim)\n",
    "        \n",
    "    #     # Concatenate: G(u) has dim-1 + dim = 2*dim-1 components\n",
    "    #     G_u = np.concatenate([grad_part, nonlinear_part], axis=1)\n",
    "        \n",
    "    #     return G_u\n",
    "\n",
    "    def forward_func(u_batch):\n",
    "        \"\"\"\n",
    "        Forward model G(u) for Allen-Cahn SPDE where V(u) = ½|G(u)|²\n",
    "        \n",
    "        Must be consistent with potential function which uses trapezoidal rule:\n",
    "        - Gradient contributions: (u[j+1] - u[j])/√h \n",
    "        - Potential contributions: √(h)(1-u_avg[j]²) where u_avg[j] = (u[j+1]+u[j])/2\n",
    "        \"\"\"\n",
    "        if u_batch.ndim == 1:\n",
    "            u_batch = u_batch.reshape(1, -1)\n",
    "        \n",
    "        # Gradient contributions: (u[j+1] - u[j])/√h for j=0..N-1\n",
    "        u_diffs = u_batch[:, 1:] - u_batch[:, :-1]  # (batch_size, dim-1)\n",
    "        grad_part = u_diffs / np.sqrt(h)\n",
    "        \n",
    "        # Potential contributions: √(h)(1-u_avg[j]²) for j=0..N-1\n",
    "        # Use midpoint rule to match potential function\n",
    "        u_left = u_batch[:, :-1]   # u[j] for j=0..N-1\n",
    "        u_right = u_batch[:, 1:]   # u[j+1] for j=0..N-1\n",
    "        u_avg = (u_left + u_right) / 2  # Midpoint values\n",
    "        nonlinear_part = np.sqrt(h) * (1 - u_avg**2)  # (batch_size, dim-1)\n",
    "        \n",
    "        # Concatenate: G(u) has (dim-1) + (dim-1) = 2*(dim-1) components\n",
    "        G_u = np.concatenate([grad_part, nonlinear_part], axis=1)\n",
    "        \n",
    "        return G_u\n",
    "    \n",
    "    # Function to compute path integral consistently\n",
    "    def compute_path_integral(path):\n",
    "        \"\"\"Efficiently calculate the path integral using vectorized operations\"\"\"\n",
    "        if path.ndim > 1:\n",
    "            return np.array([compute_path_integral(p) for p in path])\n",
    "            \n",
    "        # For a single path\n",
    "        left_points = path[:-1]\n",
    "        right_points = path[1:]\n",
    "        segment_areas = h * (left_points + right_points) / 2\n",
    "        return np.sum(segment_areas)\n",
    "    \n",
    "    # For EKM, M should be identity\n",
    "    data_dim = 2 * (dim - 1)\n",
    "    M = np.eye(data_dim)\n",
    "    \n",
    "    # Initial state - start near one of the stable states\n",
    "    initial = np.ones(dim) * 0.8 + 0.1 * np.random.randn(dim)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Define samplers to benchmark\n",
    "    samplers = {\n",
    "        \"Langevin Walk Move Ensemble\": lambda: langevin_walk_move_ensemble_dual_avg(\n",
    "            gradient_func=gradient, potential_func=potential, initial=initial, \n",
    "            n_samples=n_samples, n_warmup=burn_in, \n",
    "            n_chains_per_group=max(10, dim), h=1.362/(dim**(1/2)), \n",
    "            target_accept=0.57, n_thin=n_thin),\n",
    "        \"Ensemble Kalman Move\": lambda: ensemble_kalman_move_dual_avg(\n",
    "            forward_func=forward_func, M=M, initial=initial, \n",
    "            n_samples=n_samples, n_warmup=burn_in,\n",
    "            n_chains_per_group=max(10, dim), h=1.362/(dim**(1/2)), \n",
    "            target_accept=0.57, n_thin=n_thin, use_metropolis=True),\n",
    "    }\n",
    "    \n",
    "    for name, sampler_func in samplers.items():\n",
    "        print(f\"Running {name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            samples, acceptance_rates, step_size_history = sampler_func()\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # samples shape: (n_chains, n_samples, dim)\n",
    "            post_burn_in_samples = samples\n",
    "            \n",
    "            # Flatten samples from all chains\n",
    "            flat_samples = post_burn_in_samples.reshape(-1, dim)\n",
    "            \n",
    "            # Compute sample statistics\n",
    "            sample_mean = np.mean(flat_samples, axis=0)\n",
    "            \n",
    "            # Calculate path integrals for all samples\n",
    "            path_integrals = compute_path_integral(flat_samples)\n",
    "            mean_path_integral = np.mean(path_integrals)\n",
    "            path_integral_std = np.std(path_integrals)\n",
    "            \n",
    "            # Compute potential energies\n",
    "            potential_energies = potential(flat_samples)\n",
    "            mean_potential = np.mean(potential_energies)\n",
    "            potential_var = np.var(potential_energies)\n",
    "            \n",
    "            # Check well mixing\n",
    "            positive_well = np.mean(path_integrals > 0.5)\n",
    "            negative_well = np.mean(path_integrals < -0.5)\n",
    "            well_mixing = min(positive_well, negative_well)\n",
    "            \n",
    "            # Compute autocorrelation for path integral - following reference pattern\n",
    "            # Average over chains first, then compute path integrals\n",
    "            path_integrals_chain1 = compute_path_integral(np.mean(post_burn_in_samples, axis=0))\n",
    "            acf = autocorrelation_fft(path_integrals_chain1)\n",
    "            \n",
    "            # Compute integrated autocorrelation time\n",
    "            try:\n",
    "                tau, _, ess = integrated_autocorr_time(path_integrals_chain1)\n",
    "            except:\n",
    "                tau, ess = np.nan, np.nan\n",
    "                print(\"  Warning: Could not compute integrated autocorrelation time\")\n",
    "            \n",
    "            # Measure fraction of time spent in positive well\n",
    "            positive_fraction = np.mean(flat_samples > 0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {name}: {str(e)}\")\n",
    "            # Create dummy data in case of error\n",
    "            flat_samples = np.zeros((10, dim))\n",
    "            acceptance_rates = np.zeros(2)\n",
    "            sample_mean = np.zeros(dim)\n",
    "            mean_path_integral = np.nan\n",
    "            path_integral_std = np.nan\n",
    "            mean_potential = np.nan\n",
    "            potential_var = np.nan\n",
    "            well_mixing = np.nan\n",
    "            positive_fraction = np.nan\n",
    "            acf = np.zeros(100)\n",
    "            tau, ess = np.nan, np.nan\n",
    "            elapsed = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"samples\": flat_samples,\n",
    "            \"acceptance_rates\": acceptance_rates,\n",
    "            \"sample_mean\": sample_mean,\n",
    "            \"path_integral_mean\": mean_path_integral,\n",
    "            \"path_integral_std\": path_integral_std,\n",
    "            \"mean_potential\": mean_potential,\n",
    "            \"potential_var\": potential_var,\n",
    "            \"well_mixing\": well_mixing,\n",
    "            \"positive_fraction\": positive_fraction,\n",
    "            \"autocorrelation\": acf,\n",
    "            \"tau\": tau,\n",
    "            \"ess\": ess,\n",
    "            \"time\": elapsed\n",
    "        }\n",
    "        \n",
    "        print(f\"  Acceptance rate: {np.mean(acceptance_rates):.2f}\")\n",
    "        print(f\"  Path integral mean: {mean_path_integral:.4f}\")\n",
    "        print(f\"  Path integral std: {path_integral_std:.4f}\")\n",
    "        print(f\"  Well mixing rate: {well_mixing:.4f}\")\n",
    "        print(f\"  Integrated autocorrelation time: {tau:.2f}\" if np.isfinite(tau) else \"  Integrated autocorrelation time: NaN\")\n",
    "        print(f\"  Time: {elapsed:.2f} seconds\")\n",
    "\n",
    "        if save_dir:\n",
    "            np.save(os.path.join(save_dir, f\"samples_{name}_allen_cahn.npy\"), samples)\n",
    "            np.save(os.path.join(save_dir, f\"acf_{name}_allen_cahn.npy\"), acf)\n",
    "            \n",
    "    return results\n",
    "\n",
    "n_samples = 10**4\n",
    "burn_in = 10**3\n",
    "array_dim = [4, 8, 16, 32, 64]\n",
    "n_thin = 1\n",
    "# array_dim = [128]\n",
    "\n",
    "\n",
    "home = \"/scratch/yc3400/AffineInvariant/\"\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "folder = f\"benchmark_results_LWM_EKM_Allen-Cahn_test_{timestamp}\"\n",
    "\n",
    "wandb_project = \"AffineInvariant\"\n",
    "wandb_entity = 'yifanc96'\n",
    "wandb_run = wandb.init(\n",
    "        project=wandb_project,\n",
    "        entity=wandb_entity,\n",
    "        resume=None,\n",
    "        id    =None,\n",
    "        name = folder\n",
    "        )\n",
    "wandb.run.log_code(\".\")\n",
    "\n",
    "print(f'n_sample{n_samples}, burn_in{burn_in}, n_thin{n_thin}')\n",
    "    \n",
    "for dim in array_dim:\n",
    "    print(f\"dim={dim}\")\n",
    "    # Create a timestamped directory for this run\n",
    "    save_dir = os.path.join(home+folder, f\"{dim}\")\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Run benchmarks and save results\n",
    "    results = benchmark_samplers_allen_cahn(\n",
    "    N=dim, \n",
    "    n_samples=n_samples, \n",
    "    burn_in=burn_in, \n",
    "    n_thin=n_thin,\n",
    "    save_dir=save_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba75f3-6f70-4918-8a6a-d0e6f5236465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYKERNEL",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
