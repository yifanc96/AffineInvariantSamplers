{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6e3f6-249f-4944-9b65-eb1e2edbd036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">benchmark_results_LWM_EKM_Gaussian_sample10_4_n_thin_10_20250814-203944</strong> at: <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/684v4oqs' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/684v4oqs</a><br> View project at: <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250814_203944-684v4oqs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yc3400/scratch_research/gitrepo/AffineInvariantSamplers/github/AffineInvariantSamplers/other-samplers/Langevin-and-Kalman-move/wandb/run-20250814_204031-ozz6epvh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yifanc96/AffineInvariant/runs/ozz6epvh' target=\"_blank\">benchmark_results_LWM_EKM_Gaussian_sample10_4_n_thin_10_20250814-204031</a></strong> to <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/ozz6epvh' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/ozz6epvh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sample10000, burn_in1000, n_thin1\n",
      "dim=4\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.743154\n",
      "  Acceptance rate: 0.61\n",
      "  Mean MSE: 0.000017\n",
      "  Covariance MSE: 0.000579\n",
      "  Integrated autocorrelation time: 8.37\n",
      "  Time: 4.80 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.790388\n",
      "  Acceptance rate: 0.58\n",
      "  Mean MSE: 0.000658\n",
      "  Covariance MSE: 0.000543\n",
      "  Integrated autocorrelation time: 9.64\n",
      "  Time: 4.28 seconds\n",
      "dim=8\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.517924\n",
      "  Acceptance rate: 0.54\n",
      "  Mean MSE: 0.000169\n",
      "  Covariance MSE: 0.000035\n",
      "  Integrated autocorrelation time: 12.66\n",
      "  Time: 5.80 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.493587\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000263\n",
      "  Covariance MSE: 0.000001\n",
      "  Integrated autocorrelation time: 9.45\n",
      "  Time: 5.13 seconds\n",
      "dim=16\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.344144\n",
      "  Acceptance rate: 0.58\n",
      "  Mean MSE: 0.000002\n",
      "  Covariance MSE: 0.000329\n",
      "  Integrated autocorrelation time: 12.02\n",
      "  Time: 7.03 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.344536\n",
      "  Acceptance rate: 0.58\n",
      "  Mean MSE: 0.000003\n",
      "  Covariance MSE: 0.000140\n",
      "  Integrated autocorrelation time: 11.07\n",
      "  Time: 6.32 seconds\n",
      "dim=32\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.266759\n",
      "  Acceptance rate: 0.56\n",
      "  Mean MSE: 0.000016\n",
      "  Covariance MSE: 0.000001\n",
      "  Integrated autocorrelation time: 15.72\n",
      "  Time: 10.16 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.264723\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000006\n",
      "  Covariance MSE: 0.000027\n",
      "  Integrated autocorrelation time: 14.54\n",
      "  Time: 9.34 seconds\n",
      "dim=64\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.205418\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000002\n",
      "  Covariance MSE: 0.000032\n",
      "  Integrated autocorrelation time: 21.04\n",
      "  Time: 21.83 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.205785\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000002\n",
      "  Covariance MSE: 0.000187\n",
      "  Integrated autocorrelation time: 29.03\n",
      "  Time: 21.54 seconds\n",
      "dim=128\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.161598\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000002\n",
      "  Covariance MSE: 0.000017\n",
      "  Integrated autocorrelation time: 27.66\n",
      "  Time: 67.64 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.160233\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000001\n",
      "  Covariance MSE: 0.000005\n",
      "  Integrated autocorrelation time: 25.22\n",
      "  Time: 70.78 seconds\n",
      "dim=256\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.126789\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000001\n",
      "  Covariance MSE: 0.000000\n",
      "  Integrated autocorrelation time: 23.67\n",
      "  Time: 294.97 seconds\n",
      "Running Ensemble Kalman Move...\n",
      "Warmup complete. Final adapted step size: 0.127700\n",
      "  Acceptance rate: 0.57\n",
      "  Mean MSE: 0.000000\n",
      "  Covariance MSE: 0.000001\n",
      "  Integrated autocorrelation time: 25.17\n",
      "  Time: 326.46 seconds\n",
      "dim=512\n",
      "Running Langevin Walk Move Ensemble...\n",
      "Warmup complete. Final adapted step size: 0.100500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "def autocorrelation_fft(x, max_lag=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Efficiently compute autocorrelation function using FFT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    max_lag : int, optional\n",
    "        Maximum lag to compute (default: len(x)//3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if max_lag is None:\n",
    "        max_lag = min(n // 3, 20000)  # Cap at 20000 to prevent slow computation\n",
    "    \n",
    "    # Remove mean and normalize\n",
    "    x_norm = x - np.mean(x)\n",
    "    var = np.var(x_norm)\n",
    "    x_norm = x_norm / np.sqrt(var)\n",
    "    \n",
    "    # Compute autocorrelation using FFT\n",
    "    # Pad the signal with zeros to avoid circular correlation\n",
    "    fft = np.fft.fft(x_norm, n=2*n)\n",
    "    acf = np.fft.ifft(fft * np.conjugate(fft))[:n]\n",
    "    acf = acf.real / n  # Normalize\n",
    "    \n",
    "    return acf[:max_lag]\n",
    "\n",
    "def integrated_autocorr_time(x, M=5, c=10):\n",
    "    \"\"\"\n",
    "    Estimate the integrated autocorrelation time using a self-consistent window.\n",
    "    Based on the algorithm described by Goodman and Weare.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    M : int, default=5\n",
    "        Window size multiplier (typically 5-10)\n",
    "    c : int, default=10\n",
    "        Maximum lag cutoff for window determination\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tau : float\n",
    "        Integrated autocorrelation time\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    ess : float\n",
    "        Effective sample size\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    orig_x = x.copy()\n",
    "    \n",
    "    # Initial pairwise reduction if needed\n",
    "    k = 0\n",
    "    max_iterations = 10  # Prevent infinite loop\n",
    "    \n",
    "    while k < max_iterations:\n",
    "        # Calculate autocorrelation function\n",
    "        acf = autocorrelation_fft(x)\n",
    "        \n",
    "        # Calculate integrated autocorrelation time with self-consistent window\n",
    "        tau = 1.0  # Initialize with the first term\n",
    "        \n",
    "        # Find the window size where window <= M * tau\n",
    "        for window in range(1, len(acf)):\n",
    "            # Update tau with this window\n",
    "            tau_window = 1.0 + 2.0 * sum(acf[1:window+1])\n",
    "            \n",
    "            # Check window consistency: window <= M*tau\n",
    "            if window <= M * tau_window:\n",
    "                tau = tau_window\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # If we have a robust estimate, we're done\n",
    "        if n >= c * tau:\n",
    "            # Scale tau back to the original time scale: tau_0 = 2^k * tau_k\n",
    "            tau = tau * (2**k)\n",
    "            break\n",
    "            \n",
    "        # If we don't have a robust estimate, perform pairwise reduction\n",
    "        k += 1\n",
    "        n_half = len(x) // 2\n",
    "        x_new = np.zeros(n_half)\n",
    "        for i in range(n_half):\n",
    "            if 2*i + 1 < len(x):\n",
    "                x_new[i] = 0.5 * (x[2*i] + x[2*i+1])\n",
    "            else:\n",
    "                x_new[i] = x[2*i]\n",
    "        x = x_new\n",
    "        n = len(x)\n",
    "    \n",
    "    # If we exited without a robust estimate, compute one final estimate\n",
    "    if k >= max_iterations or n < c * tau:\n",
    "        acf = autocorrelation_fft(orig_x)\n",
    "        tau_reduced = 1.0 + 2.0 * sum(acf[1:min(len(acf), int(M)+1)])\n",
    "        # Scale tau back to the original time scale\n",
    "        tau = tau_reduced * (2**k)\n",
    "    \n",
    "    # Calculate effective sample size using original series length\n",
    "    ess = len(orig_x) / tau\n",
    "    \n",
    "    return tau, acf, ess\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def langevin_walk_move_ensemble_dual_avg(gradient_func, potential_func, initial, n_samples, n_chains_per_group=5, \n",
    "                                        h=0.01, n_thin=1, target_accept=0.57, n_warmup=1000,\n",
    "                                        gamma=0.05, t0=10, kappa=0.75):\n",
    "    \"\"\"\n",
    "    Vectorized Langevin Walk Move sampler using normalized ensemble preconditioning with dual averaging\n",
    "    for automatic step size adaptation.\n",
    "    \n",
    "    This version follows the mathematical description exactly and includes dual averaging to tune\n",
    "    the step size to achieve a target acceptance rate during warmup.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gradient_func : callable\n",
    "        Function that computes the gradient of the potential V(x)\n",
    "    potential_func : callable\n",
    "        Function that computes the potential V(x)\n",
    "    initial : np.ndarray\n",
    "        Initial state\n",
    "    n_samples : int\n",
    "        Number of samples to collect (after warmup)\n",
    "    n_chains_per_group : int\n",
    "        Number of chains per group (default: 5)\n",
    "    h : float\n",
    "        Initial step size (default: 0.01)\n",
    "    n_thin : int\n",
    "        Thinning factor - store every n_thin sample (default: 1, no thinning)\n",
    "    target_accept : float\n",
    "        Target acceptance rate for dual averaging (default: 0.57)\n",
    "    n_warmup : int\n",
    "        Number of warmup iterations for step size adaptation (default: 1000)\n",
    "    gamma : float\n",
    "        Dual averaging parameter controlling adaptation rate (default: 0.05)\n",
    "    t0 : float\n",
    "        Dual averaging parameter for numerical stability (default: 10)\n",
    "    kappa : float\n",
    "        Dual averaging parameter controlling decay (default: 0.75, should be in (0.5, 1])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : np.ndarray\n",
    "        Collected samples from all chains (after warmup)\n",
    "    acceptance_rates : np.ndarray\n",
    "        Final acceptance rates for all chains\n",
    "    step_size_history : np.ndarray\n",
    "        History of step sizes during adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    orig_dim = initial.shape\n",
    "    flat_dim = np.prod(orig_dim)\n",
    "    total_chains = 2 * n_chains_per_group\n",
    "    \n",
    "    # Create initial states with small random perturbations\n",
    "    states = np.tile(initial.flatten(), (total_chains, 1)) + 0.1 * np.random.randn(total_chains, flat_dim)\n",
    "    \n",
    "    # Split into two groups\n",
    "    group1 = slice(0, n_chains_per_group)\n",
    "    group2 = slice(n_chains_per_group, total_chains)\n",
    "    \n",
    "    # Dual averaging initialization\n",
    "    log_h = np.log(h)\n",
    "    log_h_bar = 0.0\n",
    "    h_bar = 0.0\n",
    "    step_size_history = []\n",
    "    \n",
    "    # Calculate total iterations needed based on thinning factor\n",
    "    total_sampling_iterations = n_samples * n_thin\n",
    "    total_iterations = n_warmup + total_sampling_iterations\n",
    "    \n",
    "    # Storage for samples and acceptance tracking\n",
    "    samples = np.zeros((total_chains, n_samples, flat_dim))\n",
    "    accepts_warmup = np.zeros(total_chains)  # Track accepts during warmup\n",
    "    accepts_sampling = np.zeros(total_chains)  # Track accepts during sampling\n",
    "    \n",
    "    # Sample index to track where to store thinned samples\n",
    "    sample_idx = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    for i in range(total_iterations):\n",
    "        is_warmup = i < n_warmup\n",
    "        current_h = h if not is_warmup else np.exp(log_h)\n",
    "        \n",
    "        # Store current state from all chains (only during sampling phase)\n",
    "        if not is_warmup and (i - n_warmup) % n_thin == 0 and sample_idx < n_samples:\n",
    "            samples[:, sample_idx] = states\n",
    "            sample_idx += 1\n",
    "        \n",
    "        # Compute normalized centered ensemble from group 2 for group 1 update\n",
    "        # B_S has shape (flat_dim, n_chains_per_group)\n",
    "        group2_centered = states[group2] - np.mean(states[group2], axis=0)\n",
    "        B_S2 = (group2_centered / np.sqrt(n_chains_per_group)).T  # (flat_dim, n_chains_per_group)\n",
    "        \n",
    "        # First group update\n",
    "        current_q1 = states[group1].copy()\n",
    "        current_q1_reshaped = current_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U1 = potential_func(current_q1_reshaped)\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad1 = gradient_func(current_q1_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "        \n",
    "        # Generate noise in ensemble space (dimension n_chains_per_group)\n",
    "        z1 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        # Langevin proposal: x_i = x_i - h * B_S * B_S^T * ∇V(x_i) + √(2h) * B_S * z_i\n",
    "        # where z_i has dimension n_chains_per_group\n",
    "        \n",
    "        # Compute B_S^T * B_S (covariance in parameter space)\n",
    "        cov_param2 = np.dot(B_S2, B_S2.T)  # (flat_dim, flat_dim)\n",
    "        \n",
    "        # Drift term: -h * B_S * B_S^T * ∇V\n",
    "        drift_term = -current_h * (cov_param2 @ grad1.T).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        # Noise term: √(2h) * B_S * z\n",
    "        noise_term = np.sqrt(2 * current_h) * (B_S2 @ z1.T).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        proposed_q1 = current_q1 + drift_term + noise_term\n",
    "        \n",
    "        # Compute proposed energy\n",
    "        proposed_q1_reshaped = proposed_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        proposed_U1 = potential_func(proposed_q1_reshaped)\n",
    "        \n",
    "        # Metropolis-Hastings acceptance with correct proposal probabilities\n",
    "        grad1_proposed = gradient_func(proposed_q1_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad1_proposed = np.nan_to_num(grad1_proposed, nan=0.0)\n",
    "        \n",
    "        # Forward and reverse proposal means\n",
    "        mean_forward = current_q1 - current_h * (cov_param2 @ grad1.T).T\n",
    "        mean_reverse = proposed_q1 - current_h * (cov_param2 @ grad1_proposed.T).T\n",
    "        \n",
    "        # Compute residuals\n",
    "        residual_forward = proposed_q1 - mean_forward\n",
    "        residual_reverse = current_q1 - mean_reverse\n",
    "        \n",
    "        # Compute quadratic forms with covariance 2h * B_S * B_S^T\n",
    "        # The inverse covariance is (2h * B_S * B_S^T)^(-1) = (1/2h) * (B_S * B_S^T)^(-1)\n",
    "        \n",
    "        try:\n",
    "            # Add regularization for numerical stability\n",
    "            reg_cov2 = cov_param2 + 1e-8 * np.eye(flat_dim)\n",
    "            L2 = np.linalg.cholesky(reg_cov2)\n",
    "            \n",
    "            # Solve for quadratic forms: (1/2h) * residual^T * (B_S * B_S^T)^(-1) * residual\n",
    "            Y_forward = np.linalg.solve(L2, residual_forward.T)  # (flat_dim, n_chains_per_group)\n",
    "            Y_reverse = np.linalg.solve(L2, residual_reverse.T)  # (flat_dim, n_chains_per_group)\n",
    "            \n",
    "            # Quadratic forms: (1/(4h)) * residual^T * (B_S * B_S^T)^(-1) * residual\n",
    "            # Note: factor is 1/(4h), not 1/(2h), to match covariance 2h * B_S * B_S^T\n",
    "            log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "            log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback to pseudoinverse\n",
    "            inv_cov2 = np.linalg.pinv(cov_param2)\n",
    "            \n",
    "            log_q_forward = np.zeros(n_chains_per_group)\n",
    "            log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            for j in range(n_chains_per_group):\n",
    "                log_q_forward[j] = -residual_forward[j] @ inv_cov2 @ residual_forward[j] / (4 * current_h)\n",
    "                log_q_reverse[j] = -residual_reverse[j] @ inv_cov2 @ residual_reverse[j] / (4 * current_h)\n",
    "        \n",
    "        # Metropolis-Hastings ratio\n",
    "        dU1 = proposed_U1 - current_U1\n",
    "        log_ratio = -dU1 + log_q_reverse - log_q_forward\n",
    "        \n",
    "        # Accept/reject with numerical stability\n",
    "        accept_probs1 = np.ones_like(log_ratio)\n",
    "        exp_needed = log_ratio < 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "            accept_probs1[exp_needed] = np.exp(safe_log_ratio)\n",
    "        \n",
    "        accepts1 = np.random.random(n_chains_per_group) < accept_probs1\n",
    "        states[group1][accepts1] = proposed_q1[accepts1]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group1] += accepts1\n",
    "        else:\n",
    "            accepts_sampling[group1] += accepts1\n",
    "        \n",
    "        # Second group update using ensemble from group 1\n",
    "        group1_centered = states[group1] - np.mean(states[group1], axis=0)\n",
    "        B_S1 = (group1_centered / np.sqrt(n_chains_per_group)).T  # (flat_dim, n_chains_per_group)\n",
    "        \n",
    "        current_q2 = states[group2].copy()\n",
    "        current_q2_reshaped = current_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U2 = potential_func(current_q2_reshaped)\n",
    "        \n",
    "        grad2 = gradient_func(current_q2_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "        \n",
    "        z2 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        cov_param1 = np.dot(B_S1, B_S1.T)  # (flat_dim, flat_dim)\n",
    "        \n",
    "        drift_term = -current_h * (cov_param1 @ grad2.T).T\n",
    "        noise_term = np.sqrt(2 * current_h) * (B_S1 @ z2.T).T\n",
    "        \n",
    "        proposed_q2 = current_q2 + drift_term + noise_term\n",
    "        \n",
    "        proposed_q2_reshaped = proposed_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        proposed_U2 = potential_func(proposed_q2_reshaped)\n",
    "        \n",
    "        grad2_proposed = gradient_func(proposed_q2_reshaped).reshape(n_chains_per_group, -1)\n",
    "        grad2_proposed = np.nan_to_num(grad2_proposed, nan=0.0)\n",
    "        \n",
    "        mean_forward = current_q2 - current_h * (cov_param1 @ grad2.T).T\n",
    "        mean_reverse = proposed_q2 - current_h * (cov_param1 @ grad2_proposed.T).T\n",
    "        \n",
    "        residual_forward = proposed_q2 - mean_forward\n",
    "        residual_reverse = current_q2 - mean_reverse\n",
    "        \n",
    "        try:\n",
    "            reg_cov1 = cov_param1 + 1e-8 * np.eye(flat_dim)\n",
    "            L1 = np.linalg.cholesky(reg_cov1)\n",
    "            \n",
    "            Y_forward = np.linalg.solve(L1, residual_forward.T)\n",
    "            Y_reverse = np.linalg.solve(L1, residual_reverse.T)\n",
    "            \n",
    "            log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "            log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            inv_cov1 = np.linalg.pinv(cov_param1)\n",
    "            \n",
    "            log_q_forward = np.zeros(n_chains_per_group)\n",
    "            log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            for j in range(n_chains_per_group):\n",
    "                log_q_forward[j] = -residual_forward[j] @ inv_cov1 @ residual_forward[j] / (4 * current_h)\n",
    "                log_q_reverse[j] = -residual_reverse[j] @ inv_cov1 @ residual_reverse[j] / (4 * current_h)\n",
    "        \n",
    "        dU2 = proposed_U2 - current_U2\n",
    "        log_ratio = -dU2 + log_q_reverse - log_q_forward\n",
    "        \n",
    "        accept_probs2 = np.ones_like(log_ratio)\n",
    "        exp_needed = log_ratio < 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "            accept_probs2[exp_needed] = np.exp(safe_log_ratio)\n",
    "        \n",
    "        accepts2 = np.random.random(n_chains_per_group) < accept_probs2\n",
    "        states[group2][accepts2] = proposed_q2[accepts2]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group2] += accepts2\n",
    "        else:\n",
    "            accepts_sampling[group2] += accepts2\n",
    "        \n",
    "        # Dual averaging step size adaptation during warmup\n",
    "        if is_warmup:\n",
    "            # Average acceptance probability across all chains in this iteration\n",
    "            current_accept_rate = (np.sum(accepts1) + np.sum(accepts2)) / total_chains\n",
    "            \n",
    "            # Dual averaging update\n",
    "            m = i + 1  # iteration number (1-indexed)\n",
    "            eta_m = 1.0 / (m + t0)\n",
    "            \n",
    "            # Update log step size\n",
    "            h_bar = (1 - eta_m) * h_bar + eta_m * (target_accept - current_accept_rate)\n",
    "            \n",
    "            # Compute log step size with shrinkage\n",
    "            log_h = np.log(h) - np.sqrt(m) / gamma * h_bar\n",
    "            \n",
    "            # Update log_h_bar for final step size\n",
    "            eta_bar_m = m**(-kappa)\n",
    "            log_h_bar = (1 - eta_bar_m) * log_h_bar + eta_bar_m * log_h\n",
    "            \n",
    "            # Store step size history\n",
    "            step_size_history.append(np.exp(log_h))\n",
    "        \n",
    "        # After warmup, fix step size to the adapted value\n",
    "        if i == n_warmup - 1:\n",
    "            h = np.exp(log_h_bar)\n",
    "            print(f\"Warmup complete. Final adapted step size: {h:.6f}\")\n",
    "    \n",
    "    # Reshape final samples to original dimensions\n",
    "    samples = samples.reshape((total_chains, n_samples) + orig_dim)\n",
    "    \n",
    "    # Compute acceptance rates for sampling phase only\n",
    "    acceptance_rates = accepts_sampling / total_sampling_iterations\n",
    "    \n",
    "    return samples, acceptance_rates, np.array(step_size_history)\n",
    "\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "def ensemble_kalman_move_dual_avg(forward_func, initial, n_samples, M=None,\n",
    "                                 n_chains_per_group=5, h=0.01, n_thin=1, use_metropolis=True,\n",
    "                                 target_accept=0.57, n_warmup=1000, gamma=0.05, t0=10, kappa=0.75):\n",
    "    \"\"\"\n",
    "    Ensemble Kalman Move sampler for least squares type densities with dual averaging\n",
    "    for automatic step size adaptation.\n",
    "    \n",
    "    For density π(x) ∝ exp(-V(x)) with V(x) = ½G(x)ᵀMG(x) where G: ℝᵈ → ℝʳ.\n",
    "    \n",
    "    The proposal is: x' = x - h * B_S * F_S^T * M * G(x) + √(2h) * B_S * z\n",
    "    where:\n",
    "    - B_S: normalized centered ensemble from other group (flat_dim, n_chains_per_group)\n",
    "    - F_S: normalized centered G(x) from other group (data_dim, n_chains_per_group)  \n",
    "    - z ~ N(0, I_{n_chains_per_group × n_chains_per_group})\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    forward_func : callable\n",
    "        Function G(x) that maps parameters to data space ℝᵈ → ℝʳ\n",
    "        Must accept batch input: G(x_batch) where x_batch has shape (n_batch, *param_shape)\n",
    "        Returns array of shape (n_batch, data_dim)\n",
    "    initial : np.ndarray\n",
    "        Initial state\n",
    "    n_samples : int\n",
    "        Number of samples to collect (after warmup)\n",
    "    M : np.ndarray, optional\n",
    "        Precision matrix in data space (default: identity)\n",
    "    n_chains_per_group : int\n",
    "        Number of chains per group (default: 5)\n",
    "    h : float\n",
    "        Initial step size (default: 0.01)\n",
    "    n_thin : int\n",
    "        Thinning factor (default: 1, no thinning)\n",
    "    use_metropolis : bool\n",
    "        Whether to use Metropolis correction for exact sampling (default: True)\n",
    "    target_accept : float\n",
    "        Target acceptance rate for dual averaging (default: 0.57)\n",
    "    n_warmup : int\n",
    "        Number of warmup iterations for step size adaptation (default: 1000)\n",
    "    gamma : float\n",
    "        Dual averaging parameter controlling adaptation rate (default: 0.05)\n",
    "    t0 : float\n",
    "        Dual averaging parameter for numerical stability (default: 10)\n",
    "    kappa : float\n",
    "        Dual averaging parameter controlling decay (default: 0.75, should be in (0.5, 1])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : np.ndarray\n",
    "        Collected samples from all chains (after warmup)\n",
    "    acceptance_rates : np.ndarray\n",
    "        Final acceptance rates for all chains\n",
    "    step_size_history : np.ndarray\n",
    "        History of step sizes during adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    orig_dim = initial.shape\n",
    "    flat_dim = np.prod(orig_dim)\n",
    "    total_chains = 2 * n_chains_per_group\n",
    "    \n",
    "    # Create initial states with small random perturbations\n",
    "    states = np.tile(initial.flatten(), (total_chains, 1)) + 0.1 * np.random.randn(total_chains, flat_dim)\n",
    "    \n",
    "    # Split into two groups\n",
    "    group1 = slice(0, n_chains_per_group)\n",
    "    group2 = slice(n_chains_per_group, total_chains)\n",
    "    \n",
    "    # Dual averaging initialization\n",
    "    log_h = np.log(h)\n",
    "    log_h_bar = 0.0\n",
    "    h_bar = 0.0\n",
    "    step_size_history = []\n",
    "    \n",
    "    # Calculate total iterations needed based on thinning factor\n",
    "    total_sampling_iterations = n_samples * n_thin\n",
    "    total_iterations = n_warmup + total_sampling_iterations\n",
    "    \n",
    "    # Storage for samples and acceptance tracking\n",
    "    samples = np.zeros((total_chains, n_samples, flat_dim))\n",
    "    accepts_warmup = np.zeros(total_chains)  # Track accepts during warmup\n",
    "    accepts_sampling = np.zeros(total_chains)  # Track accepts during sampling\n",
    "    \n",
    "    # Sample index to track where to store thinned samples\n",
    "    sample_idx = 0\n",
    "    \n",
    "    # Determine data dimension from first forward model evaluation\n",
    "    test_G = forward_func(initial)\n",
    "    data_dim = len(test_G)\n",
    "    \n",
    "    # Set default M = I if not provided\n",
    "    if M is None:\n",
    "        M = np.eye(data_dim)\n",
    "    \n",
    "    # Main sampling loop\n",
    "    for i in range(total_iterations):\n",
    "        is_warmup = i < n_warmup\n",
    "        current_h = h if not is_warmup else np.exp(log_h)\n",
    "        \n",
    "        # Store current state from all chains (only during sampling phase)\n",
    "        if not is_warmup and (i - n_warmup) % n_thin == 0 and sample_idx < n_samples:\n",
    "            samples[:, sample_idx] = states\n",
    "            sample_idx += 1\n",
    "        \n",
    "        # Update group 1 using group 2 for ensemble information\n",
    "        group2_reshaped = states[group2].reshape(n_chains_per_group, *orig_dim)\n",
    "        G_group2 = forward_func(group2_reshaped)  # (n_chains_per_group, data_dim)\n",
    "        mean_G2 = np.mean(G_group2, axis=0)  # (data_dim,)\n",
    "        \n",
    "        # F_S1 = (1/√(N/2)) * [G(x_group2) - mean_G2]^T ∈ ℝ^{data_dim × n_chains_per_group}\n",
    "        F_S1 = ((G_group2 - mean_G2) / np.sqrt(n_chains_per_group)).T  # (data_dim, n_chains_per_group)\n",
    "        \n",
    "        # B_S1 from group 2: normalized centered ensemble in parameter space\n",
    "        group2_centered = states[group2] - np.mean(states[group2], axis=0)\n",
    "        B_S1 = (group2_centered / np.sqrt(n_chains_per_group)).T  # (flat_dim, n_chains_per_group)\n",
    "        \n",
    "        # Vectorized update for group 1\n",
    "        current_q1 = states[group1].copy()\n",
    "        current_q1_reshaped = current_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        G_current1 = forward_func(current_q1_reshaped)  # (n_chains_per_group, data_dim)\n",
    "        current_U1 = 0.5 * np.sum(G_current1 * (G_current1 @ M), axis=1)  # (n_chains_per_group,)\n",
    "        \n",
    "        # Generate noise z ~ N(0, I_{n_chains_per_group × n_chains_per_group})\n",
    "        z1 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        # Ensemble Kalman proposal:\n",
    "        # x' = x - h * B_S1 * F_S1^T * M * G(x) + √(2h) * B_S1 * z\n",
    "        \n",
    "        # Drift term: -h * B_S1 * F_S1^T * M * G(x) (vectorized)\n",
    "        MG_current1 = G_current1 @ M  # (n_chains_per_group, data_dim)\n",
    "        drift_terms = -current_h * (B_S1 @ (F_S1.T @ MG_current1.T)).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        # Noise term: √(2h) * B_S1 * z (vectorized)\n",
    "        noise_terms = np.sqrt(2 * current_h) * (B_S1 @ z1.T).T  # (n_chains_per_group, flat_dim)\n",
    "        \n",
    "        # Proposed states (vectorized)\n",
    "        proposed_q1 = current_q1 + drift_terms + noise_terms\n",
    "        \n",
    "        accepts1 = np.zeros(n_chains_per_group, dtype=bool)\n",
    "        \n",
    "        if use_metropolis:\n",
    "            # Metropolis-Hastings correction\n",
    "            proposed_q1_reshaped = proposed_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "            G_proposed1 = forward_func(proposed_q1_reshaped)\n",
    "            proposed_U1 = 0.5 * np.sum(G_proposed1 * (G_proposed1 @ M), axis=1)\n",
    "            \n",
    "            # For correct Metropolis step, we need to compute proposal probabilities\n",
    "            # The proposal has the form: x' ~ N(μ_forward, Σ) where:\n",
    "            # μ_forward = x - h * B_S1 * F_S1^T * M * G(x)\n",
    "            # Σ = 2h * B_S1 * B_S1^T (covariance of noise term)\n",
    "            \n",
    "            # IMPORTANT: F_S1 is FIXED (from group 2) during this proposal step\n",
    "            # So both forward and reverse proposals use the SAME F_S1\n",
    "            \n",
    "            # Forward proposal mean (what we used to generate proposed_q1)\n",
    "            mean_forward = current_q1 + drift_terms  # Note: drift_terms already has negative sign\n",
    "            \n",
    "            # Reverse proposal: proposed_q1 -> current_q1\n",
    "            # Mean: proposed_q1 - h * B_S1 * F_S1^T * M * G(proposed_q1)\n",
    "            # Note: Uses SAME F_S1 (from group 2), not recomputed!\n",
    "            MG_proposed1 = G_proposed1 @ M\n",
    "            reverse_drift = -current_h * (B_S1 @ (F_S1.T @ MG_proposed1.T)).T  # Same F_S1!\n",
    "            mean_reverse = proposed_q1 + reverse_drift\n",
    "            \n",
    "            # Compute residuals\n",
    "            residual_forward = proposed_q1 - mean_forward  # Should be = noise_terms\n",
    "            residual_reverse = current_q1 - mean_reverse\n",
    "            \n",
    "            # Proposal covariance: The noise term √(2h) * B_S * z has covariance 2h * B_S * B_S^T\n",
    "            # But for the log probability calculation, we factor out the h:\n",
    "            # log q = -½ * residual^T * (2h * B_S * B_S^T)^(-1) * residual\n",
    "            #       = -1/(4h) * residual^T * (B_S * B_S^T)^(-1) * residual\n",
    "            # So we use cov = B_S * B_S^T (no h) and divide by 4h later\n",
    "            cov_proposal = np.dot(B_S1, B_S1.T)  # (flat_dim, flat_dim) - NO h here!\n",
    "            \n",
    "            try:\n",
    "                # Use Cholesky for numerical stability\n",
    "                reg_cov = cov_proposal + 1e-8 * np.eye(flat_dim)\n",
    "                L = np.linalg.cholesky(reg_cov)\n",
    "                \n",
    "                # Compute log proposal probabilities: log q(x' | x) = -½(x' - μ)^T Σ^(-1) (x' - μ)\n",
    "                # Since Σ = 2h * B_S1 * B_S1^T, we have:\n",
    "                # log q = -¼h * residual^T * (B_S1 * B_S1^T)^(-1) * residual\n",
    "                \n",
    "                # Solve L * y = residual^T for each chain\n",
    "                Y_forward = np.linalg.solve(L, residual_forward.T)  # (flat_dim, n_chains_per_group)\n",
    "                Y_reverse = np.linalg.solve(L, residual_reverse.T)   # (flat_dim, n_chains_per_group)\n",
    "                \n",
    "                # Log proposal probabilities (factor of 1/(4h) because Σ = 2h * ...)\n",
    "                log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "                log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "                \n",
    "            except np.linalg.LinAlgError:\n",
    "                # If Cholesky fails, use simpler Metropolis (energy only)\n",
    "                log_q_forward = np.zeros(n_chains_per_group)\n",
    "                log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            # Metropolis-Hastings ratio\n",
    "            dU1 = proposed_U1 - current_U1\n",
    "            log_ratio = -dU1 + log_q_reverse - log_q_forward\n",
    "            \n",
    "            # Accept/reject with numerical stability\n",
    "            accept_probs1 = np.ones_like(log_ratio)\n",
    "            exp_needed = log_ratio < 0\n",
    "            if np.any(exp_needed):\n",
    "                safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "                accept_probs1[exp_needed] = np.exp(safe_log_ratio)\n",
    "            \n",
    "            accepts1 = np.random.random(n_chains_per_group) < accept_probs1\n",
    "            states[group1][accepts1] = proposed_q1[accepts1]\n",
    "        else:\n",
    "            # Pure Ensemble Kalman (no Metropolis correction)\n",
    "            states[group1] = proposed_q1\n",
    "            accepts1 = np.ones(n_chains_per_group, dtype=bool)  # Always accept\n",
    "        \n",
    "        # Track acceptances for group 1\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group1] += accepts1\n",
    "        else:\n",
    "            accepts_sampling[group1] += accepts1\n",
    "        \n",
    "        # Update group 2 using group 1 (symmetric structure)\n",
    "        group1_reshaped = states[group1].reshape(n_chains_per_group, *orig_dim)\n",
    "        G_group1 = forward_func(group1_reshaped)\n",
    "        mean_G1 = np.mean(G_group1, axis=0)\n",
    "        F_S0 = ((G_group1 - mean_G1) / np.sqrt(n_chains_per_group)).T\n",
    "        \n",
    "        group1_centered = states[group1] - np.mean(states[group1], axis=0)\n",
    "        B_S0 = (group1_centered / np.sqrt(n_chains_per_group)).T\n",
    "        \n",
    "        current_q2 = states[group2].copy()\n",
    "        current_q2_reshaped = current_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        G_current2 = forward_func(current_q2_reshaped)\n",
    "        current_U2 = 0.5 * np.sum(G_current2 * (G_current2 @ M), axis=1)\n",
    "        \n",
    "        z2 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        MG_current2 = G_current2 @ M\n",
    "        drift_terms = -current_h * (B_S0 @ (F_S0.T @ MG_current2.T)).T\n",
    "        noise_terms = np.sqrt(2 * current_h) * (B_S0 @ z2.T).T\n",
    "        proposed_q2 = current_q2 + drift_terms + noise_terms\n",
    "        \n",
    "        accepts2 = np.zeros(n_chains_per_group, dtype=bool)\n",
    "        \n",
    "        if use_metropolis:\n",
    "            proposed_q2_reshaped = proposed_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "            G_proposed2 = forward_func(proposed_q2_reshaped)\n",
    "            proposed_U2 = 0.5 * np.sum(G_proposed2 * (G_proposed2 @ M), axis=1)\n",
    "            \n",
    "            mean_forward = current_q2 + drift_terms\n",
    "            \n",
    "            # Reverse proposal: proposed_q2 -> current_q2  \n",
    "            # Mean: proposed_q2 - h * B_S0 * F_S0^T * M * G(proposed_q2)\n",
    "            # Note: Uses SAME F_S0 (from group 1), not recomputed!\n",
    "            MG_proposed2 = G_proposed2 @ M\n",
    "            reverse_drift = -current_h * (B_S0 @ (F_S0.T @ MG_proposed2.T)).T  # Same F_S0!\n",
    "            mean_reverse = proposed_q2 + reverse_drift\n",
    "            \n",
    "            residual_forward = proposed_q2 - mean_forward\n",
    "            residual_reverse = current_q2 - mean_reverse\n",
    "            \n",
    "            cov_proposal = np.dot(B_S0, B_S0.T)  # NO h here!\n",
    "            \n",
    "            try:\n",
    "                reg_cov = cov_proposal + 1e-8 * np.eye(flat_dim)\n",
    "                L = np.linalg.cholesky(reg_cov)\n",
    "                \n",
    "                Y_forward = np.linalg.solve(L, residual_forward.T)\n",
    "                Y_reverse = np.linalg.solve(L, residual_reverse.T)\n",
    "                \n",
    "                log_q_forward = -np.sum(Y_forward**2, axis=0) / (4 * current_h)\n",
    "                log_q_reverse = -np.sum(Y_reverse**2, axis=0) / (4 * current_h)\n",
    "                \n",
    "            except np.linalg.LinAlgError:\n",
    "                log_q_forward = np.zeros(n_chains_per_group)\n",
    "                log_q_reverse = np.zeros(n_chains_per_group)\n",
    "            \n",
    "            dU2 = proposed_U2 - current_U2\n",
    "            log_ratio = -dU2 + log_q_reverse - log_q_forward\n",
    "            \n",
    "            accept_probs2 = np.ones_like(log_ratio)\n",
    "            exp_needed = log_ratio < 0\n",
    "            if np.any(exp_needed):\n",
    "                safe_log_ratio = np.clip(log_ratio[exp_needed], -100, None)\n",
    "                accept_probs2[exp_needed] = np.exp(safe_log_ratio)\n",
    "            \n",
    "            accepts2 = np.random.random(n_chains_per_group) < accept_probs2\n",
    "            states[group2][accepts2] = proposed_q2[accepts2]\n",
    "        else:\n",
    "            states[group2] = proposed_q2\n",
    "            accepts2 = np.ones(n_chains_per_group, dtype=bool)\n",
    "        \n",
    "        # Track acceptances for group 2\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group2] += accepts2\n",
    "        else:\n",
    "            accepts_sampling[group2] += accepts2\n",
    "        \n",
    "        # Dual averaging step size adaptation during warmup\n",
    "        if is_warmup:\n",
    "            # Average acceptance probability across all chains in this iteration\n",
    "            current_accept_rate = (np.sum(accepts1) + np.sum(accepts2)) / total_chains\n",
    "            \n",
    "            # Dual averaging update\n",
    "            m = i + 1  # iteration number (1-indexed)\n",
    "            eta_m = 1.0 / (m + t0)\n",
    "            \n",
    "            # Update log step size\n",
    "            h_bar = (1 - eta_m) * h_bar + eta_m * (target_accept - current_accept_rate)\n",
    "            \n",
    "            # Compute log step size with shrinkage\n",
    "            log_h = np.log(h) - np.sqrt(m) / gamma * h_bar\n",
    "            \n",
    "            # Update log_h_bar for final step size\n",
    "            eta_bar_m = m**(-kappa)\n",
    "            log_h_bar = (1 - eta_bar_m) * log_h_bar + eta_bar_m * log_h\n",
    "            \n",
    "            # Store step size history\n",
    "            step_size_history.append(np.exp(log_h))\n",
    "        \n",
    "        # After warmup, fix step size to the adapted value\n",
    "        if i == n_warmup - 1:\n",
    "            h = np.exp(log_h_bar)\n",
    "            print(f\"Warmup complete. Final adapted step size: {h:.6f}\")\n",
    "    \n",
    "    # Reshape final samples to original dimensions\n",
    "    samples = samples.reshape((total_chains, n_samples) + orig_dim)\n",
    "    \n",
    "    # Compute acceptance rates for sampling phase only\n",
    "    acceptance_rates = accepts_sampling / total_sampling_iterations\n",
    "    \n",
    "    return samples, acceptance_rates, np.array(step_size_history)\n",
    "\n",
    "def create_high_dim_precision(dim, condition_number=100):\n",
    "    \"\"\"Create a high-dimensional diagonal precision matrix with given condition number.\"\"\"\n",
    "    # For reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create diagonal eigenvalues with desired condition number\n",
    "    eigenvalues = 0.1 * np.linspace(1, condition_number, dim)\n",
    "    \n",
    "    # For diagonal matrices, we can just return the eigenvalues\n",
    "    # This avoids storing the full matrix which is mostly zeros\n",
    "    return eigenvalues\n",
    "\n",
    "def benchmark_samplers(dim=40, n_samples=10000, burn_in=1000, condition_number=100, n_thin = 1, save_dir = None):\n",
    "    \"\"\"\n",
    "    Benchmark different MCMC samplers on a high-dimensional Gaussian.\n",
    "    \"\"\"\n",
    "    # Create precision matrix (inverse covariance) - just the diagonal values\n",
    "    precision_diag = create_high_dim_precision(dim, condition_number)\n",
    "    \n",
    "    # Compute covariance matrix diagonal for reference (needed for evaluation)\n",
    "    # For diagonal matrices, inverse is just reciprocal of diagonal elements\n",
    "    cov_diag = 1.0 / precision_diag\n",
    "    \n",
    "    true_mean = np.ones(dim)\n",
    "    \n",
    "    def log_density(x):\n",
    "        \"\"\"Optimized log density of the multivariate Gaussian with diagonal precision\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        # Vectorized operation for all samples using broadcasting\n",
    "        centered = x - true_mean\n",
    "        # For diagonal precision, this simplifies to sum of elementwise products\n",
    "        # This avoids the expensive einsum operation\n",
    "        result = -0.5 * np.sum(centered**2 * precision_diag, axis=1)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def gradient(x):\n",
    "        \"\"\"Optimized gradient of the negative log density with diagonal precision\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        # Vectorized operation for all samples\n",
    "        centered = x - true_mean\n",
    "        # For diagonal precision, this is just elementwise multiplication\n",
    "        # This avoids the expensive einsum operation\n",
    "        result = centered * precision_diag[np.newaxis, :]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def potential(x):\n",
    "        \"\"\"Optimized negative log density (potential energy) with diagonal precision\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        # Vectorized operation for all samples\n",
    "        centered = x - true_mean\n",
    "        # For diagonal precision, this simplifies to sum of elementwise products\n",
    "        result = 0.5 * np.sum(centered**2 * precision_diag, axis=1)\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def forward_func(x_batch):\n",
    "        \"\"\"\n",
    "        Forward model G(x) = x - μ\n",
    "        x_batch: (n_batch, dim)\n",
    "        returns: (n_batch, dim)  # data dimension = parameter dimension\n",
    "        \"\"\"\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch.reshape(1, -1)\n",
    "        \n",
    "        # G(x) = x - μ\n",
    "        return x_batch - true_mean  # (n_batch, dim)\n",
    "    \n",
    "    # For this construction, M = Λ (precision matrix)\n",
    "    M = np.diag(precision_diag)\n",
    "    \n",
    "    # Initial state\n",
    "    initial = np.zeros(dim)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Define samplers to benchmark with burn-in\n",
    "    total_samples = n_samples\n",
    "    \n",
    "    \n",
    "    # Define samplers to benchmark - adjust parameters for high-dimensional case\n",
    "    samplers = {\n",
    "        \"Langevin Walk Move Ensemble\": lambda: langevin_walk_move_ensemble_dual_avg(gradient_func=gradient, potential_func=potential, initial=initial, n_samples=total_samples, n_warmup=burn_in, n_chains_per_group=dim, h=1.362/(dim**(1/2)), target_accept=0.57, n_thin=n_thin),\n",
    "        \"Ensemble Kalman Move\": lambda: ensemble_kalman_move_dual_avg(forward_func=forward_func, M = M, initial=initial, n_samples=total_samples, n_warmup=burn_in, n_chains_per_group=dim, h=1.362/(dim**(1/2)), target_accept=0.57, n_thin=n_thin, use_metropolis=True),\n",
    "    }\n",
    "    \n",
    "    for name, sampler_func in samplers.items():\n",
    "        print(f\"Running {name}...\")\n",
    "        start_time = time.time()\n",
    "        samples, acceptance_rates, step_size_history = sampler_func()\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        post_burn_in_samples = samples\n",
    "        \n",
    "        # Flatten samples from all chains\n",
    "        flat_samples = post_burn_in_samples.reshape(-1, dim)\n",
    "        \n",
    "        # Compute sample mean and covariance\n",
    "        sample_mean = np.mean(flat_samples, axis=0)\n",
    "        \n",
    "        # For MSE calculation, we don't need to compute the full covariance matrix\n",
    "        # We can compute the diagonal elements directly\n",
    "        sample_var = np.var(flat_samples, axis=0)\n",
    "        \n",
    "        # Calculate mean squared error for mean and covariance\n",
    "        mean_mse = np.mean((sample_mean - true_mean)**2) / np.mean(true_mean**2)\n",
    "        # For diagonal covariance, we only compare diagonal elements\n",
    "        cov_mse = np.sum((sample_var - cov_diag)**2) / np.sum(cov_diag**2)\n",
    "        \n",
    "        # Compute autocorrelation for first dimension\n",
    "        # Average over chains to compute autocorrelation\n",
    "        acf = autocorrelation_fft(np.mean(post_burn_in_samples[:, :, 0], axis=0))\n",
    "        \n",
    "        # Compute integrated autocorrelation time for first dimension\n",
    "        try:\n",
    "            tau, _, ess = integrated_autocorr_time(np.mean(post_burn_in_samples[:, :, 0], axis=0))\n",
    "        except:\n",
    "            tau, ess = np.nan, np.nan\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"samples\": flat_samples,\n",
    "            \"acceptance_rates\": acceptance_rates,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"cov_mse\": cov_mse,\n",
    "            \"autocorrelation\": acf,\n",
    "            \"tau\": tau,\n",
    "            \"ess\": ess,\n",
    "            \"time\": elapsed\n",
    "        }\n",
    "        \n",
    "        print(f\"  Acceptance rate: {np.mean(acceptance_rates):.2f}\")\n",
    "        print(f\"  Mean MSE: {mean_mse:.6f}\")\n",
    "        print(f\"  Covariance MSE: {cov_mse:.6f}\")\n",
    "        print(f\"  Integrated autocorrelation time: {tau:.2f}\")\n",
    "        print(f\"  Time: {elapsed:.2f} seconds\")\n",
    "\n",
    "        if save_dir:\n",
    "            np.save(os.path.join(save_dir, f\"samples_{name}.npy\"), post_burn_in_samples)\n",
    "            np.save(os.path.join(save_dir, f\"acf_{name}.npy\"), acf)\n",
    "            \n",
    "    return results, true_mean, cov_diag\n",
    "    \n",
    "\n",
    "n_samples = 10**4\n",
    "burn_in = 10**3\n",
    "array_dim = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "n_thin = 1\n",
    "# array_dim = [128]\n",
    "\n",
    "\n",
    "home = \"/scratch/yc3400/AffineInvariant/\"\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "folder = f\"benchmark_results_LWM_EKM_Gaussian_sample10_4_n_thin_10_{timestamp}\"\n",
    "\n",
    "wandb_project = \"AffineInvariant\"\n",
    "wandb_entity = 'yifanc96'\n",
    "wandb_run = wandb.init(\n",
    "        project=wandb_project,\n",
    "        entity=wandb_entity,\n",
    "        resume=None,\n",
    "        id    =None,\n",
    "        name = folder\n",
    "        )\n",
    "wandb.run.log_code(\".\")\n",
    "\n",
    "print(f'n_sample{n_samples}, burn_in{burn_in}, n_thin{n_thin}')\n",
    "    \n",
    "for dim in array_dim:\n",
    "    print(f\"dim={dim}\")\n",
    "    # Create a timestamped directory for this run\n",
    "    save_dir = os.path.join(home+folder, f\"{dim}\")\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Run benchmarks and save results\n",
    "    results, true_mean, cov_diag = benchmark_samplers(\n",
    "    dim=dim, \n",
    "    n_samples=n_samples, \n",
    "    burn_in=burn_in, \n",
    "    condition_number=1000,\n",
    "    n_thin=n_thin,\n",
    "    save_dir=save_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba75f3-6f70-4918-8a6a-d0e6f5236465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYKERNEL",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
