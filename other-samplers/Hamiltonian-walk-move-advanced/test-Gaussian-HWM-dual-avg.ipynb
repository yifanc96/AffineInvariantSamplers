{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c49533d-3d05-49d1-92be-b28483d6e705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">benchmark_results_HWMdualavg_Gaussian_sample_20250823-151751</strong> at: <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/oor42r9m' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/oor42r9m</a><br> View project at: <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250823_151752-oor42r9m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yc3400/scratch_research/gitrepo/AffineInvariantSamplers/github/AffineInvariantSamplers/other-samplers/Hamiltonian-walk-move-advanced/wandb/run-20250823_152117-3ne22y8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yifanc96/AffineInvariant/runs/3ne22y8j' target=\"_blank\">benchmark_results_HWMdualavg_Gaussian_sample_20250823-152117</a></strong> to <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/3ne22y8j' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/3ne22y8j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sample10000, burn_in1000, n_thin1\n",
      "dim=4\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 1.149099\n",
      "  Acceptance rate: 0.65\n",
      "  Mean MSE: 0.000065\n",
      "  Covariance MSE: 0.000221\n",
      "  Integrated autocorrelation time: 4.75\n",
      "  Time: 4.67 seconds\n",
      "dim=8\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.967865\n",
      "  Acceptance rate: 0.67\n",
      "  Mean MSE: 0.000002\n",
      "  Covariance MSE: 0.000002\n",
      "  Integrated autocorrelation time: 2.51\n",
      "  Time: 5.40 seconds\n",
      "dim=16\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.845588\n",
      "  Acceptance rate: 0.65\n",
      "  Mean MSE: 0.000004\n",
      "  Covariance MSE: 0.000002\n",
      "  Integrated autocorrelation time: 2.38\n",
      "  Time: 6.07 seconds\n",
      "dim=32\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.737972\n",
      "  Acceptance rate: 0.64\n",
      "  Mean MSE: 0.000001\n",
      "  Covariance MSE: 0.000002\n",
      "  Integrated autocorrelation time: 2.23\n",
      "  Time: 7.95 seconds\n",
      "dim=64\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.674633\n",
      "  Acceptance rate: 0.65\n",
      "  Mean MSE: 0.000000\n",
      "  Covariance MSE: 0.000004\n",
      "  Integrated autocorrelation time: 2.32\n",
      "  Time: 15.70 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "def autocorrelation_fft(x, max_lag=None):\n",
    "    \"\"\"\n",
    "    Efficiently compute autocorrelation function using FFT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    max_lag : int, optional\n",
    "        Maximum lag to compute (default: len(x)//3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if max_lag is None:\n",
    "        max_lag = min(n // 3, 20000)  # Cap at 20000 to prevent slow computation\n",
    "    \n",
    "    # Remove mean and normalize\n",
    "    x_norm = x - np.mean(x)\n",
    "    var = np.var(x_norm)\n",
    "    x_norm = x_norm / np.sqrt(var)\n",
    "    \n",
    "    # Compute autocorrelation using FFT\n",
    "    # Pad the signal with zeros to avoid circular correlation\n",
    "    fft = np.fft.fft(x_norm, n=2*n)\n",
    "    acf = np.fft.ifft(fft * np.conjugate(fft))[:n]\n",
    "    acf = acf.real / n  # Normalize\n",
    "    \n",
    "    return acf[:max_lag]\n",
    "\n",
    "def integrated_autocorr_time(x, M=5, c=10):\n",
    "    \"\"\"\n",
    "    Estimate the integrated autocorrelation time using a self-consistent window.\n",
    "    Based on the algorithm described by Goodman and Weare.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    M : int, default=5\n",
    "        Window size multiplier (typically 5-10)\n",
    "    c : int, default=10\n",
    "        Maximum lag cutoff for window determination\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tau : float\n",
    "        Integrated autocorrelation time\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    ess : float\n",
    "        Effective sample size\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    orig_x = x.copy()\n",
    "    \n",
    "    # Initial pairwise reduction if needed\n",
    "    k = 0\n",
    "    max_iterations = 10  # Prevent infinite loop\n",
    "    \n",
    "    while k < max_iterations:\n",
    "        # Calculate autocorrelation function\n",
    "        acf = autocorrelation_fft(x)\n",
    "        \n",
    "        # Calculate integrated autocorrelation time with self-consistent window\n",
    "        tau = 1.0  # Initialize with the first term\n",
    "        \n",
    "        # Find the window size where window <= M * tau\n",
    "        for window in range(1, len(acf)):\n",
    "            # Update tau with this window\n",
    "            tau_window = 1.0 + 2.0 * sum(acf[1:window+1])\n",
    "            \n",
    "            # Check window consistency: window <= M*tau\n",
    "            if window <= M * tau_window:\n",
    "                tau = tau_window\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # If we have a robust estimate, we're done\n",
    "        if n >= c * tau:\n",
    "            # Scale tau back to the original time scale: tau_0 = 2^k * tau_k\n",
    "            tau = tau * (2**k)\n",
    "            break\n",
    "            \n",
    "        # If we don't have a robust estimate, perform pairwise reduction\n",
    "        k += 1\n",
    "        n_half = len(x) // 2\n",
    "        x_new = np.zeros(n_half)\n",
    "        for i in range(n_half):\n",
    "            if 2*i + 1 < len(x):\n",
    "                x_new[i] = 0.5 * (x[2*i] + x[2*i+1])\n",
    "            else:\n",
    "                x_new[i] = x[2*i]\n",
    "        x = x_new\n",
    "        n = len(x)\n",
    "    \n",
    "    # If we exited without a robust estimate, compute one final estimate\n",
    "    if k >= max_iterations or n < c * tau:\n",
    "        acf = autocorrelation_fft(orig_x)\n",
    "        tau_reduced = 1.0 + 2.0 * sum(acf[1:min(len(acf), int(M)+1)])\n",
    "        # Scale tau back to the original time scale\n",
    "        tau = tau_reduced * (2**k)\n",
    "    \n",
    "    # Calculate effective sample size using original series length\n",
    "    ess = len(orig_x) / tau\n",
    "    \n",
    "    return tau, acf, ess\n",
    "\n",
    "\n",
    "def hamiltonian_walk_move_dual_avg(gradient_func, potential_func, initial, n_samples, \n",
    "                                  n_chains_per_group=5, epsilon_init=0.01, n_leapfrog=10, \n",
    "                                  beta=0.05, n_thin=1, target_accept=0.65, n_warmup=1000,\n",
    "                                  gamma=0.05, t0=10, kappa=0.75):\n",
    "    \"\"\"\n",
    "    Hamiltonian Walk Move sampler with dual averaging for automatic step size adaptation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gradient_func : callable\n",
    "        Function that computes gradients of the log probability\n",
    "    potential_func : callable  \n",
    "        Function that computes the negative log probability (potential energy)\n",
    "    initial : array_like\n",
    "        Initial state\n",
    "    n_samples : int\n",
    "        Number of samples to collect (after warmup)\n",
    "    n_chains_per_group : int\n",
    "        Number of chains per group (default: 5)\n",
    "    epsilon_init : float\n",
    "        Initial step size (default: 0.01)\n",
    "    n_leapfrog : int\n",
    "        Number of leapfrog steps (default: 10)\n",
    "    beta : float\n",
    "        Preconditioning parameter (default: 0.05)\n",
    "    n_thin : int\n",
    "        Thinning factor - store every n_thin sample (default: 1, no thinning)\n",
    "    target_accept : float\n",
    "        Target acceptance rate for dual averaging (default: 0.65)\n",
    "    n_warmup : int\n",
    "        Number of warmup iterations for step size adaptation (default: 1000)\n",
    "    gamma : float\n",
    "        Dual averaging parameter controlling adaptation rate (default: 0.05)\n",
    "    t0 : float\n",
    "        Dual averaging parameter for numerical stability (default: 10)\n",
    "    kappa : float\n",
    "        Dual averaging parameter controlling decay (default: 0.75, should be in (0.5, 1])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : ndarray\n",
    "        Generated samples from all chains (after warmup)\n",
    "    acceptance_rates : ndarray\n",
    "        Final acceptance rates for each chain\n",
    "    step_size_history : ndarray\n",
    "        History of step sizes during adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    orig_dim = initial.shape\n",
    "    flat_dim = np.prod(orig_dim)\n",
    "    total_chains = 2 * n_chains_per_group\n",
    "    \n",
    "    # Create initial states with small random perturbations\n",
    "    states = np.tile(initial.flatten(), (total_chains, 1)) + 0.1 * np.random.randn(total_chains, flat_dim)\n",
    "    \n",
    "    # Split into two groups\n",
    "    group1 = slice(0, n_chains_per_group)\n",
    "    group2 = slice(n_chains_per_group, total_chains)\n",
    "    \n",
    "    # Dual averaging initialization\n",
    "    log_epsilon = np.log(epsilon_init)\n",
    "    log_epsilon_bar = 0.0\n",
    "    H_bar = 0.0\n",
    "    step_size_history = []\n",
    "    \n",
    "    # Calculate total iterations needed based on thinning factor\n",
    "    total_sampling_iterations = n_samples * n_thin\n",
    "    total_iterations = n_warmup + total_sampling_iterations\n",
    "    \n",
    "    # Storage for samples and acceptance tracking\n",
    "    samples = np.zeros((total_chains, n_samples, flat_dim))\n",
    "    accepts_warmup = np.zeros(total_chains)  # Track accepts during warmup\n",
    "    accepts_sampling = np.zeros(total_chains)  # Track accepts during sampling\n",
    "    \n",
    "    # Sample index to track where to store thinned samples\n",
    "    sample_idx = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    for i in range(total_iterations):\n",
    "        is_warmup = i < n_warmup\n",
    "        current_epsilon = epsilon_init if not is_warmup else np.exp(log_epsilon)\n",
    "        \n",
    "        # Store current state from all chains (only during sampling phase)\n",
    "        if not is_warmup and (i - n_warmup) % n_thin == 0 and sample_idx < n_samples:\n",
    "            samples[:, sample_idx] = states\n",
    "            sample_idx += 1\n",
    "        \n",
    "        # Precompute step size terms\n",
    "        beta_eps = beta * current_epsilon\n",
    "        beta_eps_half = beta_eps / 2\n",
    "        \n",
    "        # Compute centered ensembles for preconditioning\n",
    "        centered2 = (states[group2] - np.mean(states[group2], axis=0)) / np.sqrt(n_chains_per_group)\n",
    "        \n",
    "        # First group update\n",
    "        p1 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        # Store current state and energy\n",
    "        current_q1 = states[group1].copy()\n",
    "        current_q1_reshaped = current_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U1 = potential_func(current_q1_reshaped)\n",
    "        current_K1 = np.clip(0.5 * np.sum(p1**2, axis=1), 0, 1000)\n",
    "        \n",
    "        # Leapfrog integration with preconditioning\n",
    "        q1 = current_q1.copy()\n",
    "        p1_current = p1.copy()\n",
    "        \n",
    "        # Initial half-step for momentum\n",
    "        grad1 = gradient_func(q1.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "        p1_current -= beta_eps_half * np.dot(grad1, centered2.T)\n",
    "        \n",
    "        # Full leapfrog steps\n",
    "        for step in range(n_leapfrog):\n",
    "            # Position update with ensemble preconditioning\n",
    "            q1 += beta_eps * np.dot(p1_current, centered2)\n",
    "            \n",
    "            if step < n_leapfrog - 1:\n",
    "                # Momentum update\n",
    "                grad1 = gradient_func(q1.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "                grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "                p1_current -= beta_eps * np.dot(grad1, centered2.T)\n",
    "        \n",
    "        # Final half-step for momentum\n",
    "        grad1 = gradient_func(q1.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "        p1_current -= beta_eps_half * np.dot(grad1, centered2.T)\n",
    "        \n",
    "        # Compute proposed energy\n",
    "        proposed_U1 = potential_func(q1.reshape(n_chains_per_group, *orig_dim))\n",
    "        proposed_K1 = np.clip(0.5 * np.sum(p1_current**2, axis=1), 0, 1000)\n",
    "        \n",
    "        # Metropolis acceptance with numerical stability\n",
    "        dH1 = (proposed_U1 + proposed_K1) - (current_U1 + current_K1)\n",
    "        \n",
    "        accept_probs1 = np.ones_like(dH1)\n",
    "        exp_needed = dH1 > 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_dH = np.clip(dH1[exp_needed], None, 100)\n",
    "            accept_probs1[exp_needed] = np.exp(-safe_dH)\n",
    "        \n",
    "        accepts1 = np.random.random(n_chains_per_group) < accept_probs1\n",
    "        states[group1][accepts1] = q1[accepts1]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group1] += accepts1\n",
    "        else:\n",
    "            accepts_sampling[group1] += accepts1\n",
    "        \n",
    "        # Second group update\n",
    "        centered1 = (states[group1] - np.mean(states[group1], axis=0)) / np.sqrt(n_chains_per_group)\n",
    "        \n",
    "        p2 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        current_q2 = states[group2].copy()\n",
    "        current_q2_reshaped = current_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U2 = potential_func(current_q2_reshaped)\n",
    "        current_K2 = np.clip(0.5 * np.sum(p2**2, axis=1), 0, 1000)\n",
    "        \n",
    "        q2 = current_q2.copy()\n",
    "        p2_current = p2.copy()\n",
    "        \n",
    "        # Initial half-step for momentum\n",
    "        grad2 = gradient_func(q2.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "        p2_current -= beta_eps_half * np.dot(grad2, centered1.T)\n",
    "        \n",
    "        # Full leapfrog steps\n",
    "        for step in range(n_leapfrog):\n",
    "            q2 += beta_eps * np.dot(p2_current, centered1)\n",
    "            \n",
    "            if step < n_leapfrog - 1:\n",
    "                grad2 = gradient_func(q2.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "                grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "                p2_current -= beta_eps * np.dot(grad2, centered1.T)\n",
    "        \n",
    "        # Final half-step for momentum\n",
    "        grad2 = gradient_func(q2.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "        p2_current -= beta_eps_half * np.dot(grad2, centered1.T)\n",
    "        \n",
    "        # Compute proposed energy\n",
    "        proposed_U2 = potential_func(q2.reshape(n_chains_per_group, *orig_dim))\n",
    "        proposed_K2 = np.clip(0.5 * np.sum(p2_current**2, axis=1), 0, 1000)\n",
    "        \n",
    "        # Metropolis acceptance\n",
    "        dH2 = (proposed_U2 + proposed_K2) - (current_U2 + current_K2)\n",
    "        \n",
    "        accept_probs2 = np.ones_like(dH2)\n",
    "        exp_needed = dH2 > 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_dH = np.clip(dH2[exp_needed], None, 100)\n",
    "            accept_probs2[exp_needed] = np.exp(-safe_dH)\n",
    "        \n",
    "        accepts2 = np.random.random(n_chains_per_group) < accept_probs2\n",
    "        states[group2][accepts2] = q2[accepts2]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group2] += accepts2\n",
    "        else:\n",
    "            accepts_sampling[group2] += accepts2\n",
    "        \n",
    "        # Dual averaging step size adaptation during warmup\n",
    "        if is_warmup:\n",
    "            # Average acceptance probability across all chains in this iteration\n",
    "            current_accept_rate = (np.sum(accepts1) + np.sum(accepts2)) / total_chains\n",
    "            \n",
    "            # Dual averaging update\n",
    "            m = i + 1  # iteration number (1-indexed)\n",
    "            eta = 1.0 / (m + t0)\n",
    "            \n",
    "            # Update log step size\n",
    "            H_bar = (1 - eta) * H_bar + eta * (target_accept - current_accept_rate)\n",
    "            \n",
    "            # Compute log step size with shrinkage\n",
    "            log_epsilon = np.log(epsilon_init) - np.sqrt(m) / gamma * H_bar\n",
    "            \n",
    "            # Update log_epsilon_bar for final step size\n",
    "            eta_bar = m**(-kappa)\n",
    "            log_epsilon_bar = (1 - eta_bar) * log_epsilon_bar + eta_bar * log_epsilon\n",
    "            \n",
    "            # Store step size history\n",
    "            step_size_history.append(np.exp(log_epsilon))\n",
    "        \n",
    "        # After warmup, fix step size to the adapted value\n",
    "        if i == n_warmup - 1:\n",
    "            epsilon_init = np.exp(log_epsilon_bar)\n",
    "            print(f\"Warmup complete. Final adapted step size: {epsilon_init:.6f}\")\n",
    "    \n",
    "    # Reshape final samples to original dimensions\n",
    "    samples = samples.reshape((total_chains, n_samples) + orig_dim)\n",
    "    \n",
    "    # Compute acceptance rates for sampling phase only\n",
    "    acceptance_rates = accepts_sampling / total_sampling_iterations\n",
    "    \n",
    "    return samples, acceptance_rates, np.array(step_size_history)\n",
    "\n",
    "\n",
    "def create_high_dim_precision(dim, condition_number=100):\n",
    "    \"\"\"Create a high-dimensional diagonal precision matrix with given condition number.\"\"\"\n",
    "    # For reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create diagonal eigenvalues with desired condition number\n",
    "    eigenvalues = 0.1 * np.linspace(1, condition_number, dim)\n",
    "    \n",
    "    # For diagonal matrices, we can just return the eigenvalues\n",
    "    # This avoids storing the full matrix which is mostly zeros\n",
    "    return eigenvalues\n",
    "\n",
    "def benchmark_samplers(dim=40, n_samples=10000, burn_in=1000, condition_number=100, n_thin=1, save_dir=None):\n",
    "    \"\"\"\n",
    "    Benchmark HWM sampler on a high-dimensional Gaussian.\n",
    "    \"\"\"\n",
    "    # Create precision matrix (inverse covariance) - just the diagonal values\n",
    "    precision_diag = create_high_dim_precision(dim, condition_number)\n",
    "    \n",
    "    # Compute covariance matrix diagonal for reference (needed for evaluation)\n",
    "    # For diagonal matrices, inverse is just reciprocal of diagonal elements\n",
    "    cov_diag = 1.0 / precision_diag\n",
    "    \n",
    "    true_mean = np.ones(dim)\n",
    "    \n",
    "    def gradient(x):\n",
    "        \"\"\"Optimized gradient of the negative log density with diagonal precision\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        # Vectorized operation for all samples\n",
    "        centered = x - true_mean\n",
    "        # For diagonal precision, this is just elementwise multiplication\n",
    "        result = centered * precision_diag[np.newaxis, :]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def potential(x):\n",
    "        \"\"\"Optimized negative log density (potential energy) with diagonal precision\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        # Vectorized operation for all samples\n",
    "        centered = x - true_mean\n",
    "        # For diagonal precision, this simplifies to sum of elementwise products\n",
    "        result = 0.5 * np.sum(centered**2 * precision_diag, axis=1)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # Initial state\n",
    "    initial = np.zeros(dim)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Define samplers to benchmark with burn-in\n",
    "    total_samples = n_samples\n",
    "    \n",
    "    # Define samplers to benchmark - adjust parameters for high-dimensional case\n",
    "    samplers = {\n",
    "        \"Hamiltonian Walk Move\": lambda: hamiltonian_walk_move_dual_avg(\n",
    "            gradient_func=gradient, \n",
    "            potential_func=potential, \n",
    "            initial=initial, \n",
    "            n_samples=total_samples, \n",
    "            n_warmup=burn_in, \n",
    "            n_chains_per_group=dim, \n",
    "            epsilon_init=1/(dim**(1/4)), \n",
    "            n_leapfrog=3, \n",
    "            beta=1.0,\n",
    "            target_accept=0.65, \n",
    "            n_thin=n_thin\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    for name, sampler_func in samplers.items():\n",
    "        print(f\"Running {name}...\")\n",
    "        start_time = time.time()\n",
    "        samples, acceptance_rates, step_size_history = sampler_func()\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        post_burn_in_samples = samples\n",
    "        \n",
    "        # Flatten samples from all chains\n",
    "        flat_samples = post_burn_in_samples.reshape(-1, dim)\n",
    "        \n",
    "        # Compute sample mean and covariance\n",
    "        sample_mean = np.mean(flat_samples, axis=0)\n",
    "        \n",
    "        # For MSE calculation, we don't need to compute the full covariance matrix\n",
    "        # We can compute the diagonal elements directly\n",
    "        sample_var = np.var(flat_samples, axis=0)\n",
    "        \n",
    "        # Calculate mean squared error for mean and covariance\n",
    "        mean_mse = np.mean((sample_mean - true_mean)**2) / np.mean(true_mean**2)\n",
    "        # For diagonal covariance, we only compare diagonal elements\n",
    "        cov_mse = np.sum((sample_var - cov_diag)**2) / np.sum(cov_diag**2)\n",
    "        \n",
    "        # Compute autocorrelation for first dimension\n",
    "        # Average over chains to compute autocorrelation\n",
    "        acf = autocorrelation_fft(np.mean(post_burn_in_samples[:, :, 0], axis=0))\n",
    "        \n",
    "        # Compute integrated autocorrelation time for first dimension\n",
    "        try:\n",
    "            tau, _, ess = integrated_autocorr_time(np.mean(post_burn_in_samples[:, :, 0], axis=0))\n",
    "        except:\n",
    "            tau, ess = np.nan, np.nan\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"samples\": flat_samples,\n",
    "            \"acceptance_rates\": acceptance_rates,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"cov_mse\": cov_mse,\n",
    "            \"autocorrelation\": acf,\n",
    "            \"tau\": tau,\n",
    "            \"ess\": ess,\n",
    "            \"time\": elapsed\n",
    "        }\n",
    "        \n",
    "        print(f\"  Acceptance rate: {np.mean(acceptance_rates):.2f}\")\n",
    "        print(f\"  Mean MSE: {mean_mse:.6f}\")\n",
    "        print(f\"  Covariance MSE: {cov_mse:.6f}\")\n",
    "        print(f\"  Integrated autocorrelation time: {tau:.2f}\")\n",
    "        print(f\"  Time: {elapsed:.2f} seconds\")\n",
    "\n",
    "        if save_dir:\n",
    "            np.save(os.path.join(save_dir, f\"samples_{name}.npy\"), post_burn_in_samples)\n",
    "            np.save(os.path.join(save_dir, f\"acf_{name}.npy\"), acf)\n",
    "            \n",
    "    return results, true_mean, cov_diag\n",
    "\n",
    "\n",
    "# Main benchmark script\n",
    "n_samples = 10**4\n",
    "burn_in = 10**3\n",
    "# array_dim = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "array_dim = [4, 8, 16, 32, 64]\n",
    "n_thin = 1\n",
    "\n",
    "home = \"/scratch/yc3400/AffineInvariant/\"\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "folder = f\"benchmark_results_HWMdualavg_Gaussian_sample_{timestamp}\"\n",
    "\n",
    "wandb_project = \"AffineInvariant\"\n",
    "wandb_entity = 'yifanc96'\n",
    "wandb_run = wandb.init(\n",
    "    project=wandb_project,\n",
    "    entity=wandb_entity,\n",
    "    resume=None,\n",
    "    id=None,\n",
    "    name=folder\n",
    ")\n",
    "wandb.run.log_code(\".\")\n",
    "\n",
    "print(f'n_sample{n_samples}, burn_in{burn_in}, n_thin{n_thin}')\n",
    "    \n",
    "for dim in array_dim:\n",
    "    print(f\"dim={dim}\")\n",
    "    # Create a timestamped directory for this run\n",
    "    save_dir = os.path.join(home + folder, f\"{dim}\")\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Run benchmarks and save results\n",
    "    results, true_mean, cov_diag = benchmark_samplers(\n",
    "        dim=dim, \n",
    "        n_samples=n_samples, \n",
    "        burn_in=burn_in, \n",
    "        condition_number=1000,\n",
    "        n_thin=n_thin,\n",
    "        save_dir=save_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99e487-c72a-4ff2-845f-d42a9fb8dfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYKERNEL",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
