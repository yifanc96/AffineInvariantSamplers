{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c49533d-3d05-49d1-92be-b28483d6e705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myifanc96\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yc3400/scratch_research/gitrepo/AffineInvariantSamplers/github/AffineInvariantSamplers/other-samplers/Hamiltonian-walk-move-advanced/wandb/run-20250823_153513-jy556l05</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yifanc96/AffineInvariant/runs/jy556l05' target=\"_blank\">benchmark_results_HWM_Allen-Cahn_5000samples-thin10_20250823-153513</a></strong> to <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yifanc96/AffineInvariant' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yifanc96/AffineInvariant/runs/jy556l05' target=\"_blank\">https://wandb.ai/yifanc96/AffineInvariant/runs/jy556l05</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sample10000, burn_in1000, n_thin1\n",
      "dim=4\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.753647\n",
      "  Acceptance rate: 0.65\n",
      "  Path integral mean: 0.0045\n",
      "  Path integral std: 0.8712\n",
      "  Well mixing rate: 0.3400\n",
      "  Integrated autocorrelation time: 3.47\n",
      "  Time: 13.50 seconds\n",
      "dim=8\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.719329\n",
      "  Acceptance rate: 0.66\n",
      "  Path integral mean: -0.0005\n",
      "  Path integral std: 0.8537\n",
      "  Well mixing rate: 0.3347\n",
      "  Integrated autocorrelation time: 3.36\n",
      "  Time: 13.64 seconds\n",
      "dim=16\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.685384\n",
      "  Acceptance rate: 0.65\n",
      "  Path integral mean: 0.0022\n",
      "  Path integral std: 0.8453\n",
      "  Well mixing rate: 0.3300\n",
      "  Integrated autocorrelation time: 4.22\n",
      "  Time: 15.34 seconds\n",
      "dim=32\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.654207\n",
      "  Acceptance rate: 0.65\n",
      "  Path integral mean: -0.0014\n",
      "  Path integral std: 0.8397\n",
      "  Well mixing rate: 0.3286\n",
      "  Integrated autocorrelation time: 3.88\n",
      "  Time: 17.72 seconds\n",
      "dim=64\n",
      "Running Hamiltonian Walk Move...\n",
      "Warmup complete. Final adapted step size: 0.619848\n",
      "  Acceptance rate: 0.65\n",
      "  Path integral mean: 0.0008\n",
      "  Path integral std: 0.8349\n",
      "  Well mixing rate: 0.3268\n",
      "  Integrated autocorrelation time: 3.21\n",
      "  Time: 32.98 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "def autocorrelation_fft(x, max_lag=None):\n",
    "    \"\"\"\n",
    "    Efficiently compute autocorrelation function using FFT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    max_lag : int, optional\n",
    "        Maximum lag to compute (default: len(x)//3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if max_lag is None:\n",
    "        max_lag = min(n // 3, 20000)  # Cap at 20000 to prevent slow computation\n",
    "    \n",
    "    # Remove mean and normalize\n",
    "    x_norm = x - np.mean(x)\n",
    "    var = np.var(x_norm)\n",
    "    x_norm = x_norm / np.sqrt(var)\n",
    "    \n",
    "    # Compute autocorrelation using FFT\n",
    "    # Pad the signal with zeros to avoid circular correlation\n",
    "    fft = np.fft.fft(x_norm, n=2*n)\n",
    "    acf = np.fft.ifft(fft * np.conjugate(fft))[:n]\n",
    "    acf = acf.real / n  # Normalize\n",
    "    \n",
    "    return acf[:max_lag]\n",
    "\n",
    "def integrated_autocorr_time(x, M=5, c=10):\n",
    "    \"\"\"\n",
    "    Estimate the integrated autocorrelation time using a self-consistent window.\n",
    "    Based on the algorithm described by Goodman and Weare.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        1D array of samples\n",
    "    M : int, default=5\n",
    "        Window size multiplier (typically 5-10)\n",
    "    c : int, default=10\n",
    "        Maximum lag cutoff for window determination\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tau : float\n",
    "        Integrated autocorrelation time\n",
    "    acf : array\n",
    "        Autocorrelation function values\n",
    "    ess : float\n",
    "        Effective sample size\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    orig_x = x.copy()\n",
    "    \n",
    "    # Initial pairwise reduction if needed\n",
    "    k = 0\n",
    "    max_iterations = 10  # Prevent infinite loop\n",
    "    \n",
    "    while k < max_iterations:\n",
    "        # Calculate autocorrelation function\n",
    "        acf = autocorrelation_fft(x)\n",
    "        \n",
    "        # Calculate integrated autocorrelation time with self-consistent window\n",
    "        tau = 1.0  # Initialize with the first term\n",
    "        \n",
    "        # Find the window size where window <= M * tau\n",
    "        for window in range(1, len(acf)):\n",
    "            # Update tau with this window\n",
    "            tau_window = 1.0 + 2.0 * sum(acf[1:window+1])\n",
    "            \n",
    "            # Check window consistency: window <= M*tau\n",
    "            if window <= M * tau_window:\n",
    "                tau = tau_window\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # If we have a robust estimate, we're done\n",
    "        if n >= c * tau:\n",
    "            # Scale tau back to the original time scale: tau_0 = 2^k * tau_k\n",
    "            tau = tau * (2**k)\n",
    "            break\n",
    "            \n",
    "        # If we don't have a robust estimate, perform pairwise reduction\n",
    "        k += 1\n",
    "        n_half = len(x) // 2\n",
    "        x_new = np.zeros(n_half)\n",
    "        for i in range(n_half):\n",
    "            if 2*i + 1 < len(x):\n",
    "                x_new[i] = 0.5 * (x[2*i] + x[2*i+1])\n",
    "            else:\n",
    "                x_new[i] = x[2*i]\n",
    "        x = x_new\n",
    "        n = len(x)\n",
    "    \n",
    "    # If we exited without a robust estimate, compute one final estimate\n",
    "    if k >= max_iterations or n < c * tau:\n",
    "        acf = autocorrelation_fft(orig_x)\n",
    "        tau_reduced = 1.0 + 2.0 * sum(acf[1:min(len(acf), int(M)+1)])\n",
    "        # Scale tau back to the original time scale\n",
    "        tau = tau_reduced * (2**k)\n",
    "    \n",
    "    # Calculate effective sample size using original series length\n",
    "    ess = len(orig_x) / tau\n",
    "    \n",
    "    return tau, acf, ess\n",
    "\n",
    "\n",
    "def hamiltonian_walk_move_dual_avg(gradient_func, potential_func, initial, n_samples, \n",
    "                                  n_chains_per_group=5, epsilon_init=0.01, n_leapfrog=10, \n",
    "                                  beta=0.05, n_thin=1, target_accept=0.65, n_warmup=1000,\n",
    "                                  gamma=0.05, t0=10, kappa=0.75):\n",
    "    \"\"\"\n",
    "    Hamiltonian Walk Move sampler with dual averaging for automatic step size adaptation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gradient_func : callable\n",
    "        Function that computes gradients of the log probability\n",
    "    potential_func : callable  \n",
    "        Function that computes the negative log probability (potential energy)\n",
    "    initial : array_like\n",
    "        Initial state\n",
    "    n_samples : int\n",
    "        Number of samples to collect (after warmup)\n",
    "    n_chains_per_group : int\n",
    "        Number of chains per group (default: 5)\n",
    "    epsilon_init : float\n",
    "        Initial step size (default: 0.01)\n",
    "    n_leapfrog : int\n",
    "        Number of leapfrog steps (default: 10)\n",
    "    beta : float\n",
    "        Preconditioning parameter (default: 0.05)\n",
    "    n_thin : int\n",
    "        Thinning factor - store every n_thin sample (default: 1, no thinning)\n",
    "    target_accept : float\n",
    "        Target acceptance rate for dual averaging (default: 0.65)\n",
    "    n_warmup : int\n",
    "        Number of warmup iterations for step size adaptation (default: 1000)\n",
    "    gamma : float\n",
    "        Dual averaging parameter controlling adaptation rate (default: 0.05)\n",
    "    t0 : float\n",
    "        Dual averaging parameter for numerical stability (default: 10)\n",
    "    kappa : float\n",
    "        Dual averaging parameter controlling decay (default: 0.75, should be in (0.5, 1])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : ndarray\n",
    "        Generated samples from all chains (after warmup)\n",
    "    acceptance_rates : ndarray\n",
    "        Final acceptance rates for each chain\n",
    "    step_size_history : ndarray\n",
    "        History of step sizes during adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    orig_dim = initial.shape\n",
    "    flat_dim = np.prod(orig_dim)\n",
    "    total_chains = 2 * n_chains_per_group\n",
    "    \n",
    "    # Create initial states with small random perturbations\n",
    "    states = np.tile(initial.flatten(), (total_chains, 1)) + 0.1 * np.random.randn(total_chains, flat_dim)\n",
    "    \n",
    "    # Split into two groups\n",
    "    group1 = slice(0, n_chains_per_group)\n",
    "    group2 = slice(n_chains_per_group, total_chains)\n",
    "    \n",
    "    # Dual averaging initialization\n",
    "    log_epsilon = np.log(epsilon_init)\n",
    "    log_epsilon_bar = 0.0\n",
    "    H_bar = 0.0\n",
    "    step_size_history = []\n",
    "    \n",
    "    # Calculate total iterations needed based on thinning factor\n",
    "    total_sampling_iterations = n_samples * n_thin\n",
    "    total_iterations = n_warmup + total_sampling_iterations\n",
    "    \n",
    "    # Storage for samples and acceptance tracking\n",
    "    samples = np.zeros((total_chains, n_samples, flat_dim))\n",
    "    accepts_warmup = np.zeros(total_chains)  # Track accepts during warmup\n",
    "    accepts_sampling = np.zeros(total_chains)  # Track accepts during sampling\n",
    "    \n",
    "    # Sample index to track where to store thinned samples\n",
    "    sample_idx = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    for i in range(total_iterations):\n",
    "        is_warmup = i < n_warmup\n",
    "        current_epsilon = epsilon_init if not is_warmup else np.exp(log_epsilon)\n",
    "        \n",
    "        # Store current state from all chains (only during sampling phase)\n",
    "        if not is_warmup and (i - n_warmup) % n_thin == 0 and sample_idx < n_samples:\n",
    "            samples[:, sample_idx] = states\n",
    "            sample_idx += 1\n",
    "        \n",
    "        # Precompute step size terms\n",
    "        beta_eps = beta * current_epsilon\n",
    "        beta_eps_half = beta_eps / 2\n",
    "        \n",
    "        # Compute centered ensembles for preconditioning\n",
    "        centered2 = (states[group2] - np.mean(states[group2], axis=0)) / np.sqrt(n_chains_per_group)\n",
    "        \n",
    "        # First group update\n",
    "        p1 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        # Store current state and energy\n",
    "        current_q1 = states[group1].copy()\n",
    "        current_q1_reshaped = current_q1.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U1 = potential_func(current_q1_reshaped)\n",
    "        current_K1 = np.clip(0.5 * np.sum(p1**2, axis=1), 0, 1000)\n",
    "        \n",
    "        # Leapfrog integration with preconditioning\n",
    "        q1 = current_q1.copy()\n",
    "        p1_current = p1.copy()\n",
    "        \n",
    "        # Initial half-step for momentum\n",
    "        grad1 = gradient_func(q1.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "        p1_current -= beta_eps_half * np.dot(grad1, centered2.T)\n",
    "        \n",
    "        # Full leapfrog steps\n",
    "        for step in range(n_leapfrog):\n",
    "            # Position update with ensemble preconditioning\n",
    "            q1 += beta_eps * np.dot(p1_current, centered2)\n",
    "            \n",
    "            if step < n_leapfrog - 1:\n",
    "                # Momentum update\n",
    "                grad1 = gradient_func(q1.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "                grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "                p1_current -= beta_eps * np.dot(grad1, centered2.T)\n",
    "        \n",
    "        # Final half-step for momentum\n",
    "        grad1 = gradient_func(q1.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad1 = np.nan_to_num(grad1, nan=0.0)\n",
    "        p1_current -= beta_eps_half * np.dot(grad1, centered2.T)\n",
    "        \n",
    "        # Compute proposed energy\n",
    "        proposed_U1 = potential_func(q1.reshape(n_chains_per_group, *orig_dim))\n",
    "        proposed_K1 = np.clip(0.5 * np.sum(p1_current**2, axis=1), 0, 1000)\n",
    "        \n",
    "        # Metropolis acceptance with numerical stability\n",
    "        dH1 = (proposed_U1 + proposed_K1) - (current_U1 + current_K1)\n",
    "        \n",
    "        accept_probs1 = np.ones_like(dH1)\n",
    "        exp_needed = dH1 > 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_dH = np.clip(dH1[exp_needed], None, 100)\n",
    "            accept_probs1[exp_needed] = np.exp(-safe_dH)\n",
    "        \n",
    "        accepts1 = np.random.random(n_chains_per_group) < accept_probs1\n",
    "        states[group1][accepts1] = q1[accepts1]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group1] += accepts1\n",
    "        else:\n",
    "            accepts_sampling[group1] += accepts1\n",
    "        \n",
    "        # Second group update\n",
    "        centered1 = (states[group1] - np.mean(states[group1], axis=0)) / np.sqrt(n_chains_per_group)\n",
    "        \n",
    "        p2 = np.random.randn(n_chains_per_group, n_chains_per_group)\n",
    "        \n",
    "        current_q2 = states[group2].copy()\n",
    "        current_q2_reshaped = current_q2.reshape(n_chains_per_group, *orig_dim)\n",
    "        current_U2 = potential_func(current_q2_reshaped)\n",
    "        current_K2 = np.clip(0.5 * np.sum(p2**2, axis=1), 0, 1000)\n",
    "        \n",
    "        q2 = current_q2.copy()\n",
    "        p2_current = p2.copy()\n",
    "        \n",
    "        # Initial half-step for momentum\n",
    "        grad2 = gradient_func(q2.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "        p2_current -= beta_eps_half * np.dot(grad2, centered1.T)\n",
    "        \n",
    "        # Full leapfrog steps\n",
    "        for step in range(n_leapfrog):\n",
    "            q2 += beta_eps * np.dot(p2_current, centered1)\n",
    "            \n",
    "            if step < n_leapfrog - 1:\n",
    "                grad2 = gradient_func(q2.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "                grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "                p2_current -= beta_eps * np.dot(grad2, centered1.T)\n",
    "        \n",
    "        # Final half-step for momentum\n",
    "        grad2 = gradient_func(q2.reshape(n_chains_per_group, *orig_dim)).reshape(n_chains_per_group, -1)\n",
    "        grad2 = np.nan_to_num(grad2, nan=0.0)\n",
    "        p2_current -= beta_eps_half * np.dot(grad2, centered1.T)\n",
    "        \n",
    "        # Compute proposed energy\n",
    "        proposed_U2 = potential_func(q2.reshape(n_chains_per_group, *orig_dim))\n",
    "        proposed_K2 = np.clip(0.5 * np.sum(p2_current**2, axis=1), 0, 1000)\n",
    "        \n",
    "        # Metropolis acceptance\n",
    "        dH2 = (proposed_U2 + proposed_K2) - (current_U2 + current_K2)\n",
    "        \n",
    "        accept_probs2 = np.ones_like(dH2)\n",
    "        exp_needed = dH2 > 0\n",
    "        if np.any(exp_needed):\n",
    "            safe_dH = np.clip(dH2[exp_needed], None, 100)\n",
    "            accept_probs2[exp_needed] = np.exp(-safe_dH)\n",
    "        \n",
    "        accepts2 = np.random.random(n_chains_per_group) < accept_probs2\n",
    "        states[group2][accepts2] = q2[accepts2]\n",
    "        \n",
    "        # Track acceptances\n",
    "        if is_warmup:\n",
    "            accepts_warmup[group2] += accepts2\n",
    "        else:\n",
    "            accepts_sampling[group2] += accepts2\n",
    "        \n",
    "        # Dual averaging step size adaptation during warmup\n",
    "        if is_warmup:\n",
    "            # Average acceptance probability across all chains in this iteration\n",
    "            current_accept_rate = (np.sum(accepts1) + np.sum(accepts2)) / total_chains\n",
    "            \n",
    "            # Dual averaging update\n",
    "            m = i + 1  # iteration number (1-indexed)\n",
    "            eta = 1.0 / (m + t0)\n",
    "            \n",
    "            # Update log step size\n",
    "            H_bar = (1 - eta) * H_bar + eta * (target_accept - current_accept_rate)\n",
    "            \n",
    "            # Compute log step size with shrinkage\n",
    "            log_epsilon = np.log(epsilon_init) - np.sqrt(m) / gamma * H_bar\n",
    "            \n",
    "            # Update log_epsilon_bar for final step size\n",
    "            eta_bar = m**(-kappa)\n",
    "            log_epsilon_bar = (1 - eta_bar) * log_epsilon_bar + eta_bar * log_epsilon\n",
    "            \n",
    "            # Store step size history\n",
    "            step_size_history.append(np.exp(log_epsilon))\n",
    "        \n",
    "        # After warmup, fix step size to the adapted value\n",
    "        if i == n_warmup - 1:\n",
    "            epsilon_init = np.exp(log_epsilon_bar)\n",
    "            print(f\"Warmup complete. Final adapted step size: {epsilon_init:.6f}\")\n",
    "    \n",
    "    # Reshape final samples to original dimensions\n",
    "    samples = samples.reshape((total_chains, n_samples) + orig_dim)\n",
    "    \n",
    "    # Compute acceptance rates for sampling phase only\n",
    "    acceptance_rates = accepts_sampling / total_sampling_iterations\n",
    "    \n",
    "    return samples, acceptance_rates, np.array(step_size_history)\n",
    "\n",
    "\n",
    "def benchmark_samplers_allen_cahn(N=100, n_samples=10000, burn_in=1000, n_thin=1, save_dir=None):\n",
    "    \"\"\"\n",
    "    Benchmark HWM sampler on the invariant measure of the Allen-Cahn SPDE.\n",
    "    \n",
    "    The Allen-Cahn SPDE has the invariant measure with density proportional to:\n",
    "    exp(-∫[1/(2h) * (du/dx)² + V(u)] dx)\n",
    "    \n",
    "    where V(u) = (1 - u²)² is the double-well potential and h is the discretization step.\n",
    "    \"\"\"\n",
    "    # Define discretization parameters\n",
    "    h = 1.0 / N\n",
    "    dim = N + 1  # Including boundary points\n",
    "    \n",
    "    # Define potential function V'(u) = -4u(1 - u^2)\n",
    "    def V_prime(u):\n",
    "        return -4 * u * (1 - u**2)\n",
    "    \n",
    "    # Define the gradient of the negative log density - vectorized\n",
    "    def gradient(u):\n",
    "        \"\"\"Numerically stable vectorized gradient of the negative log density\"\"\"\n",
    "        if u.ndim == 1:\n",
    "            u = u.reshape(1, -1)\n",
    "            \n",
    "        grad = np.zeros_like(u)\n",
    "        \n",
    "        # Handle interior points (j=1 to j=N-1) with vectorization\n",
    "        u_prev = u[:, :-2]  # u[j-1] for j=1...N-1\n",
    "        u_curr = u[:, 1:-1]  # u[j] for j=1...N-1\n",
    "        u_next = u[:, 2:]    # u[j+1] for j=1...N-1\n",
    "        \n",
    "        # Coupling term contribution: (2*u[j] - u[j-1] - u[j+1])/h\n",
    "        coupling_term = (2 * u_curr - u_prev - u_next) / h\n",
    "        \n",
    "        # Potential term contribution\n",
    "        avg_prev = (u_curr + u_prev) / 2\n",
    "        avg_next = (u_curr + u_next) / 2\n",
    "        \n",
    "        v_prime_prev = -4 * avg_prev * (1 - avg_prev**2)\n",
    "        v_prime_next = -4 * avg_next * (1 - avg_next**2)\n",
    "        \n",
    "        potential_term = h * (v_prime_prev + v_prime_next) / 4\n",
    "        \n",
    "        # Combine contributions for interior points\n",
    "        grad[:, 1:-1] = coupling_term + potential_term\n",
    "        \n",
    "        # Handle boundary points\n",
    "        u_first = u[:, 0]\n",
    "        u_second = u[:, 1]\n",
    "        grad[:, 0] = (u_first - u_second) / h + h * V_prime(u_first) / 4\n",
    "        \n",
    "        u_last = u[:, -1]\n",
    "        u_second_last = u[:, -2]\n",
    "        grad[:, -1] = (u_last - u_second_last) / h + h * V_prime(u_last) / 4\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    # Define the potential energy function - vectorized\n",
    "    def potential(u):\n",
    "        \"\"\"Numerically stable vectorized negative log density (potential energy)\"\"\"\n",
    "        if u.ndim == 1:\n",
    "            u = u.reshape(1, -1)\n",
    "            \n",
    "        u_right = u[:, 1:]\n",
    "        u_left = u[:, :-1]\n",
    "        diffs = u_right - u_left\n",
    "        \n",
    "        coupling_term = np.sum(diffs**2, axis=1) / (2*h)\n",
    "        \n",
    "        u_avg = (u_right + u_left) / 2\n",
    "        v_values = (1 - u_avg**2)**2\n",
    "        potential_term = np.sum(h * v_values / 2, axis=1)\n",
    "        \n",
    "        total_potential = np.clip(coupling_term + potential_term, -1e10, 1e10)\n",
    "            \n",
    "        return total_potential\n",
    "    \n",
    "    # Function to compute path integral consistently\n",
    "    def compute_path_integral(path):\n",
    "        \"\"\"Efficiently calculate the path integral using vectorized operations\"\"\"\n",
    "        if path.ndim > 1:\n",
    "            return np.array([compute_path_integral(p) for p in path])\n",
    "            \n",
    "        # For a single path\n",
    "        left_points = path[:-1]\n",
    "        right_points = path[1:]\n",
    "        segment_areas = h * (left_points + right_points) / 2\n",
    "        return np.sum(segment_areas)\n",
    "    \n",
    "    # Initial state - start near one of the stable states\n",
    "    initial = np.ones(dim) * 0.8 + 0.1 * np.random.randn(dim)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Define samplers to benchmark\n",
    "    samplers = {\n",
    "        \"Hamiltonian Walk Move\": lambda: hamiltonian_walk_move_dual_avg(\n",
    "            gradient_func=gradient, potential_func=potential, initial=initial, \n",
    "            n_samples=n_samples, n_warmup=burn_in, \n",
    "            n_chains_per_group=max(10, dim), epsilon_init=1/(dim**(1/4)), \n",
    "            n_leapfrog=3, beta=1.0,\n",
    "            target_accept=0.65, n_thin=n_thin),\n",
    "    }\n",
    "    \n",
    "    for name, sampler_func in samplers.items():\n",
    "        print(f\"Running {name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            samples, acceptance_rates, step_size_history = sampler_func()\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # samples shape: (n_chains, n_samples, dim)\n",
    "            post_burn_in_samples = samples\n",
    "            \n",
    "            # Flatten samples from all chains\n",
    "            flat_samples = post_burn_in_samples.reshape(-1, dim)\n",
    "            \n",
    "            # Compute sample statistics\n",
    "            sample_mean = np.mean(flat_samples, axis=0)\n",
    "            \n",
    "            # Calculate path integrals for all samples\n",
    "            path_integrals = compute_path_integral(flat_samples)\n",
    "            mean_path_integral = np.mean(path_integrals)\n",
    "            path_integral_std = np.std(path_integrals)\n",
    "            \n",
    "            # Compute potential energies\n",
    "            potential_energies = potential(flat_samples)\n",
    "            mean_potential = np.mean(potential_energies)\n",
    "            potential_var = np.var(potential_energies)\n",
    "            \n",
    "            # Check well mixing\n",
    "            positive_well = np.mean(path_integrals > 0.5)\n",
    "            negative_well = np.mean(path_integrals < -0.5)\n",
    "            well_mixing = min(positive_well, negative_well)\n",
    "            \n",
    "            # Compute autocorrelation for path integral - following reference pattern\n",
    "            # Average over chains first, then compute path integrals\n",
    "            path_integrals_chain1 = compute_path_integral(np.mean(post_burn_in_samples, axis=0))\n",
    "            acf = autocorrelation_fft(path_integrals_chain1)\n",
    "            \n",
    "            # Compute integrated autocorrelation time\n",
    "            try:\n",
    "                tau, _, ess = integrated_autocorr_time(path_integrals_chain1)\n",
    "            except:\n",
    "                tau, ess = np.nan, np.nan\n",
    "                print(\"  Warning: Could not compute integrated autocorrelation time\")\n",
    "            \n",
    "            # Measure fraction of time spent in positive well\n",
    "            positive_fraction = np.mean(flat_samples > 0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {name}: {str(e)}\")\n",
    "            # Create dummy data in case of error\n",
    "            flat_samples = np.zeros((10, dim))\n",
    "            acceptance_rates = np.zeros(2)\n",
    "            sample_mean = np.zeros(dim)\n",
    "            mean_path_integral = np.nan\n",
    "            path_integral_std = np.nan\n",
    "            mean_potential = np.nan\n",
    "            potential_var = np.nan\n",
    "            well_mixing = np.nan\n",
    "            positive_fraction = np.nan\n",
    "            acf = np.zeros(100)\n",
    "            tau, ess = np.nan, np.nan\n",
    "            elapsed = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"samples\": flat_samples,\n",
    "            \"acceptance_rates\": acceptance_rates,\n",
    "            \"sample_mean\": sample_mean,\n",
    "            \"path_integral_mean\": mean_path_integral,\n",
    "            \"path_integral_std\": path_integral_std,\n",
    "            \"mean_potential\": mean_potential,\n",
    "            \"potential_var\": potential_var,\n",
    "            \"well_mixing\": well_mixing,\n",
    "            \"positive_fraction\": positive_fraction,\n",
    "            \"autocorrelation\": acf,\n",
    "            \"tau\": tau,\n",
    "            \"ess\": ess,\n",
    "            \"time\": elapsed\n",
    "        }\n",
    "        \n",
    "        print(f\"  Acceptance rate: {np.mean(acceptance_rates):.2f}\")\n",
    "        print(f\"  Path integral mean: {mean_path_integral:.4f}\")\n",
    "        print(f\"  Path integral std: {path_integral_std:.4f}\")\n",
    "        print(f\"  Well mixing rate: {well_mixing:.4f}\")\n",
    "        print(f\"  Integrated autocorrelation time: {tau:.2f}\" if np.isfinite(tau) else \"  Integrated autocorrelation time: NaN\")\n",
    "        print(f\"  Time: {elapsed:.2f} seconds\")\n",
    "\n",
    "        if save_dir:\n",
    "            np.save(os.path.join(save_dir, f\"samples_{name}_allen_cahn.npy\"), samples)\n",
    "            np.save(os.path.join(save_dir, f\"acf_{name}_allen_cahn.npy\"), acf)\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Main benchmark script\n",
    "n_samples = 10000\n",
    "burn_in = 10**3\n",
    "array_dim = [4, 8, 16, 32, 64]\n",
    "n_thin = 1\n",
    "\n",
    "home = \"/scratch/yc3400/AffineInvariant/\"\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "folder = f\"benchmark_results_HWM_Allen-Cahn_5000samples_{timestamp}\"\n",
    "\n",
    "wandb_project = \"AffineInvariant\"\n",
    "wandb_entity = 'yifanc96'\n",
    "wandb_run = wandb.init(\n",
    "    project=wandb_project,\n",
    "    entity=wandb_entity,\n",
    "    resume=None,\n",
    "    id=None,\n",
    "    name=folder\n",
    ")\n",
    "wandb.run.log_code(\".\")\n",
    "\n",
    "print(f'n_sample{n_samples}, burn_in{burn_in}, n_thin{n_thin}')\n",
    "    \n",
    "for dim in array_dim:\n",
    "    print(f\"dim={dim}\")\n",
    "    # Create a timestamped directory for this run\n",
    "    save_dir = os.path.join(home + folder, f\"{dim}\")\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Run benchmarks and save results\n",
    "    results = benchmark_samplers_allen_cahn(\n",
    "        N=dim, \n",
    "        n_samples=n_samples, \n",
    "        burn_in=burn_in, \n",
    "        n_thin=n_thin,\n",
    "        save_dir=save_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99e487-c72a-4ff2-845f-d42a9fb8dfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYKERNEL",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
